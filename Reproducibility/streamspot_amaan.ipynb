{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash Evaluation on Streamspot Dataset:\n",
    "\n",
    "This notebook is dedicated to evaluating Flash on the Streamspot dataset, which are graph-level in nature. We employ Flash in graph-level detection mode to analyze this dataset effectively. Upon completion of the notebook execution, the results will be presented.\n",
    "\n",
    "## Dataset Access: \n",
    "- The Streamspot dataset can be accessed at the following link: [Streamspot Dataset](https://github.com/sbustreamspot/sbustreamspot-data).\n",
    "- Please download the required data files from the provided link.\n",
    "\n",
    "## Data Parsing and Execution:\n",
    "- Utilize the parser included in this notebook to process the downloaded files.\n",
    "- To obtain the evaluation results, execute all cells within this notebook.\n",
    "\n",
    "## Model Training and Execution Flexibility:\n",
    "- By default, the notebook operates using pre-trained model weights.\n",
    "- Additionally, this notebook offers the flexibility to set parameters for training Graph Neural Networks (GNNs) and word2vec models from scratch.\n",
    "- You can then utilize these freshly trained models to conduct the evaluation. \n",
    "\n",
    "Follow these guidelines for a thorough and efficient analysis of the Streamspot dataset using Flash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "F1op-CbyLuN4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tpiuser2/anaconda3/envs/flash_amaan/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/tpiuser2/anaconda3/envs/flash_amaan/lib/python3.8/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "warnings.filterwarnings('ignore')\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "import multiprocessing\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train_Gnn = False\n",
    "# Train_Word2vec = False\n",
    "\n",
    "Train_Gnn = True\n",
    "Train_Word2vec = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nM7KaeCbA_mQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import gzip\n",
    "from sklearn.manifold import TSNE\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import csv\n",
    "def show(str):\n",
    "\tprint (str + ' ' + time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(time.time())))\n",
    "\n",
    "def parse_data():\n",
    "    # os.system('tar -zxvf all.tar.gz')\n",
    "\n",
    "    show('Start processing.')\n",
    "    data = []\n",
    "    gId = -1\n",
    "    with open('all.tsv') as f:\n",
    "        tsvreader = csv.reader(f, delimiter='\\t')\n",
    "        for row in tsvreader:\n",
    "            if int(row[5]) != gId:\n",
    "                gId = int(row[5])\n",
    "                show('Graph ' + str(gId))\n",
    "                # scene = int(gId/100)+1\n",
    "                # if not osp.exists('streamspot/'+str(gId)):              ## eliminated use of 'scene'.\n",
    "                #     os.system('mkdir streamspot/'+str(gId))             ## eliminated use of 'scene'.\n",
    "                ff = open('streamspot/'+str(gId)+'.txt', 'w')           ## eliminated use of 'scene'.\n",
    "            ff.write(str(row[0])+'\\t'+str(row[1])+'\\t'+str(row[2])+'\\t'+str(row[3])+'\\t'+str(row[4])+'\\t'+str(row[5])+'\\n')\n",
    "    # os.system('rm all.tsv')\n",
    "    show('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing. 2024-11-05 18:24:20\n",
      "Graph 0 2024-11-05 18:24:20\n",
      "Graph 1 2024-11-05 18:24:21\n",
      "Graph 2 2024-11-05 18:24:21\n",
      "Graph 3 2024-11-05 18:24:22\n",
      "Graph 4 2024-11-05 18:24:22\n",
      "Graph 5 2024-11-05 18:24:23\n",
      "Graph 6 2024-11-05 18:24:23\n",
      "Graph 7 2024-11-05 18:24:24\n",
      "Graph 8 2024-11-05 18:24:24\n",
      "Graph 9 2024-11-05 18:24:24\n",
      "Graph 10 2024-11-05 18:24:25\n",
      "Graph 11 2024-11-05 18:24:25\n",
      "Graph 12 2024-11-05 18:24:26\n",
      "Graph 13 2024-11-05 18:24:26\n",
      "Graph 14 2024-11-05 18:24:27\n",
      "Graph 15 2024-11-05 18:24:27\n",
      "Graph 16 2024-11-05 18:24:28\n",
      "Graph 17 2024-11-05 18:24:28\n",
      "Graph 18 2024-11-05 18:24:29\n",
      "Graph 19 2024-11-05 18:24:30\n",
      "Graph 20 2024-11-05 18:24:30\n",
      "Graph 21 2024-11-05 18:24:30\n",
      "Graph 22 2024-11-05 18:24:30\n",
      "Graph 23 2024-11-05 18:24:31\n",
      "Graph 24 2024-11-05 18:24:31\n",
      "Graph 25 2024-11-05 18:24:31\n",
      "Graph 26 2024-11-05 18:24:31\n",
      "Graph 27 2024-11-05 18:24:32\n",
      "Graph 28 2024-11-05 18:24:32\n",
      "Graph 29 2024-11-05 18:24:33\n",
      "Graph 30 2024-11-05 18:24:33\n",
      "Graph 31 2024-11-05 18:24:33\n",
      "Graph 32 2024-11-05 18:24:34\n",
      "Graph 33 2024-11-05 18:24:34\n",
      "Graph 34 2024-11-05 18:24:35\n",
      "Graph 35 2024-11-05 18:24:35\n",
      "Graph 36 2024-11-05 18:24:36\n",
      "Graph 37 2024-11-05 18:24:36\n",
      "Graph 38 2024-11-05 18:24:37\n",
      "Graph 39 2024-11-05 18:24:37\n",
      "Graph 40 2024-11-05 18:24:38\n",
      "Graph 41 2024-11-05 18:24:39\n",
      "Graph 42 2024-11-05 18:24:39\n",
      "Graph 43 2024-11-05 18:24:40\n",
      "Graph 44 2024-11-05 18:24:40\n",
      "Graph 45 2024-11-05 18:24:40\n",
      "Graph 46 2024-11-05 18:24:40\n",
      "Graph 47 2024-11-05 18:24:41\n",
      "Graph 48 2024-11-05 18:24:41\n",
      "Graph 49 2024-11-05 18:24:41\n",
      "Graph 50 2024-11-05 18:24:42\n",
      "Graph 51 2024-11-05 18:24:42\n",
      "Graph 52 2024-11-05 18:24:42\n",
      "Graph 53 2024-11-05 18:24:42\n",
      "Graph 54 2024-11-05 18:24:42\n",
      "Graph 55 2024-11-05 18:24:43\n",
      "Graph 56 2024-11-05 18:24:43\n",
      "Graph 57 2024-11-05 18:24:44\n",
      "Graph 58 2024-11-05 18:24:44\n",
      "Graph 59 2024-11-05 18:24:45\n",
      "Graph 60 2024-11-05 18:24:45\n",
      "Graph 61 2024-11-05 18:24:46\n",
      "Graph 62 2024-11-05 18:24:46\n",
      "Graph 63 2024-11-05 18:24:47\n",
      "Graph 64 2024-11-05 18:24:48\n",
      "Graph 65 2024-11-05 18:24:48\n",
      "Graph 66 2024-11-05 18:24:48\n",
      "Graph 67 2024-11-05 18:24:49\n",
      "Graph 68 2024-11-05 18:24:49\n",
      "Graph 69 2024-11-05 18:24:49\n",
      "Graph 70 2024-11-05 18:24:50\n",
      "Graph 71 2024-11-05 18:24:50\n",
      "Graph 72 2024-11-05 18:24:50\n",
      "Graph 73 2024-11-05 18:24:51\n",
      "Graph 74 2024-11-05 18:24:51\n",
      "Graph 75 2024-11-05 18:24:51\n",
      "Graph 76 2024-11-05 18:24:51\n",
      "Graph 77 2024-11-05 18:24:52\n",
      "Graph 78 2024-11-05 18:24:52\n",
      "Graph 79 2024-11-05 18:24:52\n",
      "Graph 80 2024-11-05 18:24:53\n",
      "Graph 81 2024-11-05 18:24:53\n",
      "Graph 82 2024-11-05 18:24:53\n",
      "Graph 83 2024-11-05 18:24:54\n",
      "Graph 84 2024-11-05 18:24:54\n",
      "Graph 85 2024-11-05 18:24:54\n",
      "Graph 86 2024-11-05 18:24:54\n",
      "Graph 87 2024-11-05 18:24:54\n",
      "Graph 88 2024-11-05 18:24:55\n",
      "Graph 89 2024-11-05 18:24:55\n",
      "Graph 90 2024-11-05 18:24:55\n",
      "Graph 91 2024-11-05 18:24:56\n",
      "Graph 92 2024-11-05 18:24:56\n",
      "Graph 93 2024-11-05 18:24:56\n",
      "Graph 94 2024-11-05 18:24:57\n",
      "Graph 95 2024-11-05 18:24:57\n",
      "Graph 96 2024-11-05 18:24:57\n",
      "Graph 97 2024-11-05 18:24:57\n",
      "Graph 98 2024-11-05 18:24:58\n",
      "Graph 99 2024-11-05 18:24:58\n",
      "Graph 100 2024-11-05 18:24:59\n",
      "Graph 101 2024-11-05 18:24:59\n",
      "Graph 102 2024-11-05 18:24:59\n",
      "Graph 103 2024-11-05 18:24:59\n",
      "Graph 104 2024-11-05 18:25:00\n",
      "Graph 105 2024-11-05 18:25:00\n",
      "Graph 106 2024-11-05 18:25:00\n",
      "Graph 107 2024-11-05 18:25:00\n",
      "Graph 108 2024-11-05 18:25:00\n",
      "Graph 109 2024-11-05 18:25:00\n",
      "Graph 110 2024-11-05 18:25:01\n",
      "Graph 111 2024-11-05 18:25:01\n",
      "Graph 112 2024-11-05 18:25:01\n",
      "Graph 113 2024-11-05 18:25:01\n",
      "Graph 114 2024-11-05 18:25:01\n",
      "Graph 115 2024-11-05 18:25:02\n",
      "Graph 116 2024-11-05 18:25:02\n",
      "Graph 117 2024-11-05 18:25:02\n",
      "Graph 118 2024-11-05 18:25:02\n",
      "Graph 119 2024-11-05 18:25:02\n",
      "Graph 120 2024-11-05 18:25:03\n",
      "Graph 121 2024-11-05 18:25:03\n",
      "Graph 122 2024-11-05 18:25:03\n",
      "Graph 123 2024-11-05 18:25:03\n",
      "Graph 124 2024-11-05 18:25:03\n",
      "Graph 125 2024-11-05 18:25:03\n",
      "Graph 126 2024-11-05 18:25:03\n",
      "Graph 127 2024-11-05 18:25:03\n",
      "Graph 128 2024-11-05 18:25:03\n",
      "Graph 129 2024-11-05 18:25:03\n",
      "Graph 130 2024-11-05 18:25:04\n",
      "Graph 131 2024-11-05 18:25:04\n",
      "Graph 132 2024-11-05 18:25:04\n",
      "Graph 133 2024-11-05 18:25:04\n",
      "Graph 134 2024-11-05 18:25:04\n",
      "Graph 135 2024-11-05 18:25:04\n",
      "Graph 136 2024-11-05 18:25:04\n",
      "Graph 137 2024-11-05 18:25:04\n",
      "Graph 138 2024-11-05 18:25:04\n",
      "Graph 139 2024-11-05 18:25:04\n",
      "Graph 140 2024-11-05 18:25:04\n",
      "Graph 141 2024-11-05 18:25:05\n",
      "Graph 142 2024-11-05 18:25:05\n",
      "Graph 143 2024-11-05 18:25:05\n",
      "Graph 144 2024-11-05 18:25:05\n",
      "Graph 145 2024-11-05 18:25:05\n",
      "Graph 146 2024-11-05 18:25:05\n",
      "Graph 147 2024-11-05 18:25:05\n",
      "Graph 148 2024-11-05 18:25:05\n",
      "Graph 149 2024-11-05 18:25:05\n",
      "Graph 150 2024-11-05 18:25:05\n",
      "Graph 151 2024-11-05 18:25:06\n",
      "Graph 152 2024-11-05 18:25:06\n",
      "Graph 153 2024-11-05 18:25:06\n",
      "Graph 154 2024-11-05 18:25:06\n",
      "Graph 155 2024-11-05 18:25:06\n",
      "Graph 156 2024-11-05 18:25:06\n",
      "Graph 157 2024-11-05 18:25:06\n",
      "Graph 158 2024-11-05 18:25:06\n",
      "Graph 159 2024-11-05 18:25:06\n",
      "Graph 160 2024-11-05 18:25:06\n",
      "Graph 161 2024-11-05 18:25:06\n",
      "Graph 162 2024-11-05 18:25:07\n",
      "Graph 163 2024-11-05 18:25:07\n",
      "Graph 164 2024-11-05 18:25:07\n",
      "Graph 165 2024-11-05 18:25:07\n",
      "Graph 166 2024-11-05 18:25:07\n",
      "Graph 167 2024-11-05 18:25:07\n",
      "Graph 168 2024-11-05 18:25:07\n",
      "Graph 169 2024-11-05 18:25:07\n",
      "Graph 170 2024-11-05 18:25:07\n",
      "Graph 171 2024-11-05 18:25:07\n",
      "Graph 172 2024-11-05 18:25:07\n",
      "Graph 173 2024-11-05 18:25:07\n",
      "Graph 174 2024-11-05 18:25:08\n",
      "Graph 175 2024-11-05 18:25:08\n",
      "Graph 176 2024-11-05 18:25:08\n",
      "Graph 177 2024-11-05 18:25:08\n",
      "Graph 178 2024-11-05 18:25:08\n",
      "Graph 179 2024-11-05 18:25:08\n",
      "Graph 180 2024-11-05 18:25:08\n",
      "Graph 181 2024-11-05 18:25:08\n",
      "Graph 182 2024-11-05 18:25:08\n",
      "Graph 183 2024-11-05 18:25:08\n",
      "Graph 184 2024-11-05 18:25:08\n",
      "Graph 185 2024-11-05 18:25:09\n",
      "Graph 186 2024-11-05 18:25:09\n",
      "Graph 187 2024-11-05 18:25:09\n",
      "Graph 188 2024-11-05 18:25:09\n",
      "Graph 189 2024-11-05 18:25:09\n",
      "Graph 190 2024-11-05 18:25:09\n",
      "Graph 191 2024-11-05 18:25:09\n",
      "Graph 192 2024-11-05 18:25:09\n",
      "Graph 193 2024-11-05 18:25:09\n",
      "Graph 194 2024-11-05 18:25:09\n",
      "Graph 195 2024-11-05 18:25:09\n",
      "Graph 196 2024-11-05 18:25:09\n",
      "Graph 197 2024-11-05 18:25:10\n",
      "Graph 198 2024-11-05 18:25:10\n",
      "Graph 199 2024-11-05 18:25:10\n",
      "Graph 200 2024-11-05 18:25:10\n",
      "Graph 201 2024-11-05 18:25:10\n",
      "Graph 202 2024-11-05 18:25:10\n",
      "Graph 203 2024-11-05 18:25:11\n",
      "Graph 204 2024-11-05 18:25:11\n",
      "Graph 205 2024-11-05 18:25:11\n",
      "Graph 206 2024-11-05 18:25:11\n",
      "Graph 207 2024-11-05 18:25:12\n",
      "Graph 208 2024-11-05 18:25:12\n",
      "Graph 209 2024-11-05 18:25:12\n",
      "Graph 210 2024-11-05 18:25:12\n",
      "Graph 211 2024-11-05 18:25:13\n",
      "Graph 212 2024-11-05 18:25:13\n",
      "Graph 213 2024-11-05 18:25:13\n",
      "Graph 214 2024-11-05 18:25:13\n",
      "Graph 215 2024-11-05 18:25:14\n",
      "Graph 216 2024-11-05 18:25:14\n",
      "Graph 217 2024-11-05 18:25:14\n",
      "Graph 218 2024-11-05 18:25:14\n",
      "Graph 219 2024-11-05 18:25:15\n",
      "Graph 220 2024-11-05 18:25:15\n",
      "Graph 221 2024-11-05 18:25:15\n",
      "Graph 222 2024-11-05 18:25:16\n",
      "Graph 223 2024-11-05 18:25:16\n",
      "Graph 224 2024-11-05 18:25:16\n",
      "Graph 225 2024-11-05 18:25:16\n",
      "Graph 226 2024-11-05 18:25:17\n",
      "Graph 227 2024-11-05 18:25:17\n",
      "Graph 228 2024-11-05 18:25:17\n",
      "Graph 229 2024-11-05 18:25:17\n",
      "Graph 230 2024-11-05 18:25:18\n",
      "Graph 231 2024-11-05 18:25:18\n",
      "Graph 232 2024-11-05 18:25:18\n",
      "Graph 233 2024-11-05 18:25:18\n",
      "Graph 234 2024-11-05 18:25:19\n",
      "Graph 235 2024-11-05 18:25:19\n",
      "Graph 236 2024-11-05 18:25:19\n",
      "Graph 237 2024-11-05 18:25:20\n",
      "Graph 238 2024-11-05 18:25:20\n",
      "Graph 239 2024-11-05 18:25:20\n",
      "Graph 240 2024-11-05 18:25:20\n",
      "Graph 241 2024-11-05 18:25:21\n",
      "Graph 242 2024-11-05 18:25:21\n",
      "Graph 243 2024-11-05 18:25:21\n",
      "Graph 244 2024-11-05 18:25:22\n",
      "Graph 245 2024-11-05 18:25:22\n",
      "Graph 246 2024-11-05 18:25:22\n",
      "Graph 247 2024-11-05 18:25:22\n",
      "Graph 248 2024-11-05 18:25:23\n",
      "Graph 249 2024-11-05 18:25:23\n",
      "Graph 250 2024-11-05 18:25:23\n",
      "Graph 251 2024-11-05 18:25:23\n",
      "Graph 252 2024-11-05 18:25:24\n",
      "Graph 253 2024-11-05 18:25:24\n",
      "Graph 254 2024-11-05 18:25:24\n",
      "Graph 255 2024-11-05 18:25:24\n",
      "Graph 256 2024-11-05 18:25:25\n",
      "Graph 257 2024-11-05 18:25:25\n",
      "Graph 258 2024-11-05 18:25:25\n",
      "Graph 259 2024-11-05 18:25:25\n",
      "Graph 260 2024-11-05 18:25:26\n",
      "Graph 261 2024-11-05 18:25:26\n",
      "Graph 262 2024-11-05 18:25:26\n",
      "Graph 263 2024-11-05 18:25:27\n",
      "Graph 264 2024-11-05 18:25:27\n",
      "Graph 265 2024-11-05 18:25:27\n",
      "Graph 266 2024-11-05 18:25:27\n",
      "Graph 267 2024-11-05 18:25:28\n",
      "Graph 268 2024-11-05 18:25:28\n",
      "Graph 269 2024-11-05 18:25:28\n",
      "Graph 270 2024-11-05 18:25:28\n",
      "Graph 271 2024-11-05 18:25:29\n",
      "Graph 272 2024-11-05 18:25:29\n",
      "Graph 273 2024-11-05 18:25:29\n",
      "Graph 274 2024-11-05 18:25:30\n",
      "Graph 275 2024-11-05 18:25:30\n",
      "Graph 276 2024-11-05 18:25:30\n",
      "Graph 277 2024-11-05 18:25:30\n",
      "Graph 278 2024-11-05 18:25:31\n",
      "Graph 279 2024-11-05 18:25:31\n",
      "Graph 280 2024-11-05 18:25:31\n",
      "Graph 281 2024-11-05 18:25:32\n",
      "Graph 282 2024-11-05 18:25:32\n",
      "Graph 283 2024-11-05 18:25:32\n",
      "Graph 284 2024-11-05 18:25:32\n",
      "Graph 285 2024-11-05 18:25:33\n",
      "Graph 286 2024-11-05 18:25:33\n",
      "Graph 287 2024-11-05 18:25:33\n",
      "Graph 288 2024-11-05 18:25:33\n",
      "Graph 289 2024-11-05 18:25:34\n",
      "Graph 290 2024-11-05 18:25:34\n",
      "Graph 291 2024-11-05 18:25:34\n",
      "Graph 292 2024-11-05 18:25:35\n",
      "Graph 293 2024-11-05 18:25:35\n",
      "Graph 294 2024-11-05 18:25:35\n",
      "Graph 295 2024-11-05 18:25:35\n",
      "Graph 296 2024-11-05 18:25:36\n",
      "Graph 297 2024-11-05 18:25:36\n",
      "Graph 298 2024-11-05 18:25:36\n",
      "Graph 299 2024-11-05 18:25:36\n",
      "Graph 300 2024-11-05 18:25:37\n",
      "Graph 301 2024-11-05 18:25:37\n",
      "Graph 302 2024-11-05 18:25:37\n",
      "Graph 303 2024-11-05 18:25:37\n",
      "Graph 304 2024-11-05 18:25:37\n",
      "Graph 305 2024-11-05 18:25:37\n",
      "Graph 306 2024-11-05 18:25:37\n",
      "Graph 307 2024-11-05 18:25:37\n",
      "Graph 308 2024-11-05 18:25:37\n",
      "Graph 309 2024-11-05 18:25:37\n",
      "Graph 310 2024-11-05 18:25:37\n",
      "Graph 311 2024-11-05 18:25:37\n",
      "Graph 312 2024-11-05 18:25:38\n",
      "Graph 313 2024-11-05 18:25:38\n",
      "Graph 314 2024-11-05 18:25:38\n",
      "Graph 315 2024-11-05 18:25:38\n",
      "Graph 316 2024-11-05 18:25:38\n",
      "Graph 317 2024-11-05 18:25:38\n",
      "Graph 318 2024-11-05 18:25:38\n",
      "Graph 319 2024-11-05 18:25:38\n",
      "Graph 320 2024-11-05 18:25:38\n",
      "Graph 321 2024-11-05 18:25:38\n",
      "Graph 322 2024-11-05 18:25:38\n",
      "Graph 323 2024-11-05 18:25:38\n",
      "Graph 324 2024-11-05 18:25:38\n",
      "Graph 325 2024-11-05 18:25:38\n",
      "Graph 326 2024-11-05 18:25:38\n",
      "Graph 327 2024-11-05 18:25:39\n",
      "Graph 328 2024-11-05 18:25:39\n",
      "Graph 329 2024-11-05 18:25:39\n",
      "Graph 330 2024-11-05 18:25:39\n",
      "Graph 331 2024-11-05 18:25:39\n",
      "Graph 332 2024-11-05 18:25:39\n",
      "Graph 333 2024-11-05 18:25:39\n",
      "Graph 334 2024-11-05 18:25:39\n",
      "Graph 335 2024-11-05 18:25:39\n",
      "Graph 336 2024-11-05 18:25:39\n",
      "Graph 337 2024-11-05 18:25:39\n",
      "Graph 338 2024-11-05 18:25:39\n",
      "Graph 339 2024-11-05 18:25:39\n",
      "Graph 340 2024-11-05 18:25:39\n",
      "Graph 341 2024-11-05 18:25:39\n",
      "Graph 342 2024-11-05 18:25:40\n",
      "Graph 343 2024-11-05 18:25:40\n",
      "Graph 344 2024-11-05 18:25:40\n",
      "Graph 345 2024-11-05 18:25:40\n",
      "Graph 346 2024-11-05 18:25:40\n",
      "Graph 347 2024-11-05 18:25:40\n",
      "Graph 348 2024-11-05 18:25:40\n",
      "Graph 349 2024-11-05 18:25:40\n",
      "Graph 350 2024-11-05 18:25:40\n",
      "Graph 351 2024-11-05 18:25:40\n",
      "Graph 352 2024-11-05 18:25:40\n",
      "Graph 353 2024-11-05 18:25:40\n",
      "Graph 354 2024-11-05 18:25:40\n",
      "Graph 355 2024-11-05 18:25:40\n",
      "Graph 356 2024-11-05 18:25:40\n",
      "Graph 357 2024-11-05 18:25:41\n",
      "Graph 358 2024-11-05 18:25:41\n",
      "Graph 359 2024-11-05 18:25:41\n",
      "Graph 360 2024-11-05 18:25:41\n",
      "Graph 361 2024-11-05 18:25:41\n",
      "Graph 362 2024-11-05 18:25:41\n",
      "Graph 363 2024-11-05 18:25:41\n",
      "Graph 364 2024-11-05 18:25:41\n",
      "Graph 365 2024-11-05 18:25:41\n",
      "Graph 366 2024-11-05 18:25:41\n",
      "Graph 367 2024-11-05 18:25:41\n",
      "Graph 368 2024-11-05 18:25:41\n",
      "Graph 369 2024-11-05 18:25:41\n",
      "Graph 370 2024-11-05 18:25:41\n",
      "Graph 371 2024-11-05 18:25:41\n",
      "Graph 372 2024-11-05 18:25:42\n",
      "Graph 373 2024-11-05 18:25:42\n",
      "Graph 374 2024-11-05 18:25:42\n",
      "Graph 375 2024-11-05 18:25:42\n",
      "Graph 376 2024-11-05 18:25:42\n",
      "Graph 377 2024-11-05 18:25:42\n",
      "Graph 378 2024-11-05 18:25:42\n",
      "Graph 379 2024-11-05 18:25:42\n",
      "Graph 380 2024-11-05 18:25:42\n",
      "Graph 381 2024-11-05 18:25:42\n",
      "Graph 382 2024-11-05 18:25:42\n",
      "Graph 383 2024-11-05 18:25:42\n",
      "Graph 384 2024-11-05 18:25:42\n",
      "Graph 385 2024-11-05 18:25:42\n",
      "Graph 386 2024-11-05 18:25:42\n",
      "Graph 387 2024-11-05 18:25:43\n",
      "Graph 388 2024-11-05 18:25:43\n",
      "Graph 389 2024-11-05 18:25:43\n",
      "Graph 390 2024-11-05 18:25:43\n",
      "Graph 391 2024-11-05 18:25:43\n",
      "Graph 392 2024-11-05 18:25:43\n",
      "Graph 393 2024-11-05 18:25:43\n",
      "Graph 394 2024-11-05 18:25:43\n",
      "Graph 395 2024-11-05 18:25:43\n",
      "Graph 396 2024-11-05 18:25:43\n",
      "Graph 397 2024-11-05 18:25:43\n",
      "Graph 398 2024-11-05 18:25:43\n",
      "Graph 399 2024-11-05 18:25:43\n",
      "Graph 400 2024-11-05 18:25:43\n",
      "Graph 401 2024-11-05 18:25:44\n",
      "Graph 402 2024-11-05 18:25:45\n",
      "Graph 403 2024-11-05 18:25:46\n",
      "Graph 404 2024-11-05 18:25:47\n",
      "Graph 405 2024-11-05 18:25:48\n",
      "Graph 406 2024-11-05 18:25:49\n",
      "Graph 407 2024-11-05 18:25:49\n",
      "Graph 408 2024-11-05 18:25:50\n",
      "Graph 409 2024-11-05 18:25:51\n",
      "Graph 410 2024-11-05 18:25:52\n",
      "Graph 411 2024-11-05 18:25:53\n",
      "Graph 412 2024-11-05 18:25:54\n",
      "Graph 413 2024-11-05 18:25:54\n",
      "Graph 414 2024-11-05 18:25:55\n",
      "Graph 415 2024-11-05 18:25:56\n",
      "Graph 416 2024-11-05 18:25:57\n",
      "Graph 417 2024-11-05 18:25:58\n",
      "Graph 418 2024-11-05 18:25:59\n",
      "Graph 419 2024-11-05 18:26:00\n",
      "Graph 420 2024-11-05 18:26:01\n",
      "Graph 421 2024-11-05 18:26:02\n",
      "Graph 422 2024-11-05 18:26:03\n",
      "Graph 423 2024-11-05 18:26:03\n",
      "Graph 424 2024-11-05 18:26:04\n",
      "Graph 425 2024-11-05 18:26:05\n",
      "Graph 426 2024-11-05 18:26:06\n",
      "Graph 427 2024-11-05 18:26:06\n",
      "Graph 428 2024-11-05 18:26:07\n",
      "Graph 429 2024-11-05 18:26:08\n",
      "Graph 430 2024-11-05 18:26:09\n",
      "Graph 431 2024-11-05 18:26:09\n",
      "Graph 432 2024-11-05 18:26:10\n",
      "Graph 433 2024-11-05 18:26:11\n",
      "Graph 434 2024-11-05 18:26:12\n",
      "Graph 435 2024-11-05 18:26:12\n",
      "Graph 436 2024-11-05 18:26:13\n",
      "Graph 437 2024-11-05 18:26:14\n",
      "Graph 438 2024-11-05 18:26:14\n",
      "Graph 439 2024-11-05 18:26:15\n",
      "Graph 440 2024-11-05 18:26:16\n",
      "Graph 441 2024-11-05 18:26:16\n",
      "Graph 442 2024-11-05 18:26:17\n",
      "Graph 443 2024-11-05 18:26:18\n",
      "Graph 444 2024-11-05 18:26:18\n",
      "Graph 445 2024-11-05 18:26:19\n",
      "Graph 446 2024-11-05 18:26:20\n",
      "Graph 447 2024-11-05 18:26:21\n",
      "Graph 448 2024-11-05 18:26:22\n",
      "Graph 449 2024-11-05 18:26:22\n",
      "Graph 450 2024-11-05 18:26:23\n",
      "Graph 451 2024-11-05 18:26:24\n",
      "Graph 452 2024-11-05 18:26:24\n",
      "Graph 453 2024-11-05 18:26:25\n",
      "Graph 454 2024-11-05 18:26:25\n",
      "Graph 455 2024-11-05 18:26:26\n",
      "Graph 456 2024-11-05 18:26:27\n",
      "Graph 457 2024-11-05 18:26:27\n",
      "Graph 458 2024-11-05 18:26:28\n",
      "Graph 459 2024-11-05 18:26:29\n",
      "Graph 460 2024-11-05 18:26:30\n",
      "Graph 461 2024-11-05 18:26:31\n",
      "Graph 462 2024-11-05 18:26:32\n",
      "Graph 463 2024-11-05 18:26:32\n",
      "Graph 464 2024-11-05 18:26:33\n",
      "Graph 465 2024-11-05 18:26:34\n",
      "Graph 466 2024-11-05 18:26:35\n",
      "Graph 467 2024-11-05 18:26:35\n",
      "Graph 468 2024-11-05 18:26:36\n",
      "Graph 469 2024-11-05 18:26:36\n",
      "Graph 470 2024-11-05 18:26:37\n",
      "Graph 471 2024-11-05 18:26:38\n",
      "Graph 472 2024-11-05 18:26:38\n",
      "Graph 473 2024-11-05 18:26:39\n",
      "Graph 474 2024-11-05 18:26:40\n",
      "Graph 475 2024-11-05 18:26:40\n",
      "Graph 476 2024-11-05 18:26:41\n",
      "Graph 477 2024-11-05 18:26:41\n",
      "Graph 478 2024-11-05 18:26:42\n",
      "Graph 479 2024-11-05 18:26:43\n",
      "Graph 480 2024-11-05 18:26:43\n",
      "Graph 481 2024-11-05 18:26:44\n",
      "Graph 482 2024-11-05 18:26:45\n",
      "Graph 483 2024-11-05 18:26:45\n",
      "Graph 484 2024-11-05 18:26:46\n",
      "Graph 485 2024-11-05 18:26:47\n",
      "Graph 486 2024-11-05 18:26:47\n",
      "Graph 487 2024-11-05 18:26:48\n",
      "Graph 488 2024-11-05 18:26:48\n",
      "Graph 489 2024-11-05 18:26:49\n",
      "Graph 490 2024-11-05 18:26:50\n",
      "Graph 491 2024-11-05 18:26:50\n",
      "Graph 492 2024-11-05 18:26:51\n",
      "Graph 493 2024-11-05 18:26:52\n",
      "Graph 494 2024-11-05 18:26:53\n",
      "Graph 495 2024-11-05 18:26:53\n",
      "Graph 496 2024-11-05 18:26:54\n",
      "Graph 497 2024-11-05 18:26:55\n",
      "Graph 498 2024-11-05 18:26:55\n",
      "Graph 499 2024-11-05 18:26:56\n",
      "Graph 500 2024-11-05 18:26:56\n",
      "Graph 501 2024-11-05 18:26:57\n",
      "Graph 502 2024-11-05 18:26:58\n",
      "Graph 503 2024-11-05 18:26:59\n",
      "Graph 504 2024-11-05 18:27:00\n",
      "Graph 505 2024-11-05 18:27:01\n",
      "Graph 506 2024-11-05 18:27:01\n",
      "Graph 507 2024-11-05 18:27:02\n",
      "Graph 508 2024-11-05 18:27:03\n",
      "Graph 509 2024-11-05 18:27:03\n",
      "Graph 510 2024-11-05 18:27:05\n",
      "Graph 511 2024-11-05 18:27:05\n",
      "Graph 512 2024-11-05 18:27:06\n",
      "Graph 513 2024-11-05 18:27:06\n",
      "Graph 514 2024-11-05 18:27:07\n",
      "Graph 515 2024-11-05 18:27:08\n",
      "Graph 516 2024-11-05 18:27:08\n",
      "Graph 517 2024-11-05 18:27:09\n",
      "Graph 518 2024-11-05 18:27:09\n",
      "Graph 519 2024-11-05 18:27:10\n",
      "Graph 520 2024-11-05 18:27:11\n",
      "Graph 521 2024-11-05 18:27:12\n",
      "Graph 522 2024-11-05 18:27:12\n",
      "Graph 523 2024-11-05 18:27:13\n",
      "Graph 524 2024-11-05 18:27:14\n",
      "Graph 525 2024-11-05 18:27:14\n",
      "Graph 526 2024-11-05 18:27:15\n",
      "Graph 527 2024-11-05 18:27:16\n",
      "Graph 528 2024-11-05 18:27:16\n",
      "Graph 529 2024-11-05 18:27:17\n",
      "Graph 530 2024-11-05 18:27:18\n",
      "Graph 531 2024-11-05 18:27:18\n",
      "Graph 532 2024-11-05 18:27:19\n",
      "Graph 533 2024-11-05 18:27:20\n",
      "Graph 534 2024-11-05 18:27:21\n",
      "Graph 535 2024-11-05 18:27:21\n",
      "Graph 536 2024-11-05 18:27:22\n",
      "Graph 537 2024-11-05 18:27:22\n",
      "Graph 538 2024-11-05 18:27:23\n",
      "Graph 539 2024-11-05 18:27:24\n",
      "Graph 540 2024-11-05 18:27:24\n",
      "Graph 541 2024-11-05 18:27:25\n",
      "Graph 542 2024-11-05 18:27:26\n",
      "Graph 543 2024-11-05 18:27:26\n",
      "Graph 544 2024-11-05 18:27:27\n",
      "Graph 545 2024-11-05 18:27:28\n",
      "Graph 546 2024-11-05 18:27:28\n",
      "Graph 547 2024-11-05 18:27:29\n",
      "Graph 548 2024-11-05 18:27:30\n",
      "Graph 549 2024-11-05 18:27:31\n",
      "Graph 550 2024-11-05 18:27:31\n",
      "Graph 551 2024-11-05 18:27:32\n",
      "Graph 552 2024-11-05 18:27:32\n",
      "Graph 553 2024-11-05 18:27:33\n",
      "Graph 554 2024-11-05 18:27:33\n",
      "Graph 555 2024-11-05 18:27:34\n",
      "Graph 556 2024-11-05 18:27:35\n",
      "Graph 557 2024-11-05 18:27:35\n",
      "Graph 558 2024-11-05 18:27:36\n",
      "Graph 559 2024-11-05 18:27:36\n",
      "Graph 560 2024-11-05 18:27:37\n",
      "Graph 561 2024-11-05 18:27:38\n",
      "Graph 562 2024-11-05 18:27:38\n",
      "Graph 563 2024-11-05 18:27:39\n",
      "Graph 564 2024-11-05 18:27:40\n",
      "Graph 565 2024-11-05 18:27:40\n",
      "Graph 566 2024-11-05 18:27:41\n",
      "Graph 567 2024-11-05 18:27:42\n",
      "Graph 568 2024-11-05 18:27:42\n",
      "Graph 569 2024-11-05 18:27:43\n",
      "Graph 570 2024-11-05 18:27:44\n",
      "Graph 571 2024-11-05 18:27:44\n",
      "Graph 572 2024-11-05 18:27:45\n",
      "Graph 573 2024-11-05 18:27:45\n",
      "Graph 574 2024-11-05 18:27:46\n",
      "Graph 575 2024-11-05 18:27:48\n",
      "Graph 576 2024-11-05 18:27:48\n",
      "Graph 577 2024-11-05 18:27:49\n",
      "Graph 578 2024-11-05 18:27:50\n",
      "Graph 579 2024-11-05 18:27:51\n",
      "Graph 580 2024-11-05 18:27:52\n",
      "Graph 581 2024-11-05 18:27:53\n",
      "Graph 582 2024-11-05 18:27:53\n",
      "Graph 583 2024-11-05 18:27:54\n",
      "Graph 584 2024-11-05 18:27:54\n",
      "Graph 585 2024-11-05 18:27:55\n",
      "Graph 586 2024-11-05 18:27:55\n",
      "Graph 587 2024-11-05 18:27:56\n",
      "Graph 588 2024-11-05 18:27:57\n",
      "Graph 589 2024-11-05 18:27:58\n",
      "Graph 590 2024-11-05 18:27:59\n",
      "Graph 591 2024-11-05 18:28:00\n",
      "Graph 592 2024-11-05 18:28:00\n",
      "Graph 593 2024-11-05 18:28:01\n",
      "Graph 594 2024-11-05 18:28:02\n",
      "Graph 595 2024-11-05 18:28:02\n",
      "Graph 596 2024-11-05 18:28:03\n",
      "Graph 597 2024-11-05 18:28:04\n",
      "Graph 598 2024-11-05 18:28:04\n",
      "Graph 599 2024-11-05 18:28:05\n",
      "Done. 2024-11-05 18:28:05\n"
     ]
    }
   ],
   "source": [
    "parse_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_graph(df):\n",
    "    nodes, labels, edges = {}, {}, []\n",
    "    dummies = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        actor_id, object_id = row[\"actorID\"], row[\"objectID\"]\n",
    "        action = row[\"action\"]\n",
    "\n",
    "        for entity_id in [actor_id, object_id]:\n",
    "            nodes.setdefault(entity_id, []).append(action)\n",
    "            if entity_id == actor_id:\n",
    "                labels[entity_id] = dummies[row['actor_type']]\n",
    "            else:\n",
    "                labels[entity_id] = dummies[row['object']]\n",
    "\n",
    "        edges.append((actor_id, object_id))\n",
    "\n",
    "    features, feat_labels, edge_index, mapping = [], [], [[], []], []\n",
    "    index_map = {}\n",
    "\n",
    "    for key, value in nodes.items():\n",
    "        index_map[key] = len(features)\n",
    "        features.append(value)\n",
    "        feat_labels.append(labels[key])\n",
    "        mapping.append(key)\n",
    "\n",
    "    for source, target in edges:\n",
    "        edge_index[0].append(index_map[source])\n",
    "        edge_index[1].append(index_map[target])\n",
    "\n",
    "    return features, feat_labels, edge_index, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fmXWs1dKIzD8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import SAGEConv, GATConv\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self,in_channel,out_channel):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channel, 32, normalize=True)\n",
    "        self.conv2 = SAGEConv(32, out_channel, normalize=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YBuP_tSq94f4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3PCP6SXwZaif",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from multiprocessing import Pool\n",
    "from itertools import compress\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "class EpochSaver(CallbackAny2Vec):\n",
    "    '''Callback to save model after each epoch.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        model.save('trained_weights/streamspot/streamspot.model')\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "P8oBL8LFaeOf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EpochLogger(CallbackAny2Vec):\n",
    "    '''Callback to log information about training'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(\"Epoch #{} end\".format(self.epoch))\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Se7Ei4tAapVj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = EpochLogger()\n",
    "saver = EpochSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "Epoch #0 start\n",
      "Epoch #0 end\n",
      "Epoch #1 start\n",
      "Epoch #1 end\n",
      "Epoch #2 start\n",
      "Epoch #2 end\n",
      "Epoch #3 start\n",
      "Epoch #3 end\n",
      "Epoch #4 start\n",
      "Epoch #4 end\n",
      "Epoch #5 start\n",
      "Epoch #5 end\n",
      "Epoch #6 start\n",
      "Epoch #6 end\n",
      "Epoch #7 start\n",
      "Epoch #7 end\n",
      "Epoch #8 start\n",
      "Epoch #8 end\n",
      "Epoch #9 start\n",
      "Epoch #9 end\n",
      "Epoch #10 start\n",
      "Epoch #10 end\n",
      "Epoch #11 start\n",
      "Epoch #11 end\n",
      "Epoch #12 start\n",
      "Epoch #12 end\n",
      "Epoch #13 start\n",
      "Epoch #13 end\n",
      "Epoch #14 start\n",
      "Epoch #14 end\n",
      "Epoch #15 start\n",
      "Epoch #15 end\n",
      "Epoch #16 start\n",
      "Epoch #16 end\n",
      "Epoch #17 start\n",
      "Epoch #17 end\n",
      "Epoch #18 start\n",
      "Epoch #18 end\n",
      "Epoch #19 start\n",
      "Epoch #19 end\n",
      "Epoch #20 start\n",
      "Epoch #20 end\n",
      "Epoch #21 start\n",
      "Epoch #21 end\n",
      "Epoch #22 start\n",
      "Epoch #22 end\n",
      "Epoch #23 start\n",
      "Epoch #23 end\n",
      "Epoch #24 start\n",
      "Epoch #24 end\n",
      "Epoch #25 start\n",
      "Epoch #25 end\n",
      "Epoch #26 start\n",
      "Epoch #26 end\n",
      "Epoch #27 start\n",
      "Epoch #27 end\n",
      "Epoch #28 start\n",
      "Epoch #28 end\n",
      "Epoch #29 start\n",
      "Epoch #29 end\n",
      "Epoch #30 start\n",
      "Epoch #30 end\n",
      "Epoch #31 start\n",
      "Epoch #31 end\n",
      "Epoch #32 start\n",
      "Epoch #32 end\n",
      "Epoch #33 start\n",
      "Epoch #33 end\n",
      "Epoch #34 start\n",
      "Epoch #34 end\n",
      "Epoch #35 start\n",
      "Epoch #35 end\n",
      "Epoch #36 start\n",
      "Epoch #36 end\n",
      "Epoch #37 start\n",
      "Epoch #37 end\n",
      "Epoch #38 start\n",
      "Epoch #38 end\n",
      "Epoch #39 start\n",
      "Epoch #39 end\n",
      "Epoch #40 start\n",
      "Epoch #40 end\n",
      "Epoch #41 start\n",
      "Epoch #41 end\n",
      "Epoch #42 start\n",
      "Epoch #42 end\n",
      "Epoch #43 start\n",
      "Epoch #43 end\n",
      "Epoch #44 start\n",
      "Epoch #44 end\n",
      "Epoch #45 start\n",
      "Epoch #45 end\n",
      "Epoch #46 start\n",
      "Epoch #46 end\n",
      "Epoch #47 start\n",
      "Epoch #47 end\n",
      "Epoch #48 start\n",
      "Epoch #48 end\n",
      "Epoch #49 start\n",
      "Epoch #49 end\n",
      "Epoch #50 start\n",
      "Epoch #50 end\n",
      "Epoch #51 start\n",
      "Epoch #51 end\n",
      "Epoch #52 start\n",
      "Epoch #52 end\n",
      "Epoch #53 start\n",
      "Epoch #53 end\n",
      "Epoch #54 start\n",
      "Epoch #54 end\n",
      "Epoch #55 start\n",
      "Epoch #55 end\n",
      "Epoch #56 start\n",
      "Epoch #56 end\n",
      "Epoch #57 start\n",
      "Epoch #57 end\n",
      "Epoch #58 start\n",
      "Epoch #58 end\n",
      "Epoch #59 start\n",
      "Epoch #59 end\n",
      "Epoch #60 start\n",
      "Epoch #60 end\n",
      "Epoch #61 start\n",
      "Epoch #61 end\n",
      "Epoch #62 start\n",
      "Epoch #62 end\n",
      "Epoch #63 start\n",
      "Epoch #63 end\n",
      "Epoch #64 start\n",
      "Epoch #64 end\n",
      "Epoch #65 start\n",
      "Epoch #65 end\n",
      "Epoch #66 start\n",
      "Epoch #66 end\n",
      "Epoch #67 start\n",
      "Epoch #67 end\n",
      "Epoch #68 start\n",
      "Epoch #68 end\n",
      "Epoch #69 start\n",
      "Epoch #69 end\n",
      "Epoch #70 start\n",
      "Epoch #70 end\n",
      "Epoch #71 start\n",
      "Epoch #71 end\n",
      "Epoch #72 start\n",
      "Epoch #72 end\n",
      "Epoch #73 start\n",
      "Epoch #73 end\n",
      "Epoch #74 start\n",
      "Epoch #74 end\n",
      "Epoch #75 start\n",
      "Epoch #75 end\n",
      "Epoch #76 start\n",
      "Epoch #76 end\n",
      "Epoch #77 start\n",
      "Epoch #77 end\n",
      "Epoch #78 start\n",
      "Epoch #78 end\n",
      "Epoch #79 start\n",
      "Epoch #79 end\n",
      "Epoch #80 start\n",
      "Epoch #80 end\n",
      "Epoch #81 start\n",
      "Epoch #81 end\n",
      "Epoch #82 start\n",
      "Epoch #82 end\n",
      "Epoch #83 start\n",
      "Epoch #83 end\n",
      "Epoch #84 start\n",
      "Epoch #84 end\n",
      "Epoch #85 start\n",
      "Epoch #85 end\n",
      "Epoch #86 start\n",
      "Epoch #86 end\n",
      "Epoch #87 start\n",
      "Epoch #87 end\n",
      "Epoch #88 start\n",
      "Epoch #88 end\n",
      "Epoch #89 start\n",
      "Epoch #89 end\n",
      "Epoch #90 start\n",
      "Epoch #90 end\n",
      "Epoch #91 start\n",
      "Epoch #91 end\n",
      "Epoch #92 start\n",
      "Epoch #92 end\n",
      "Epoch #93 start\n",
      "Epoch #93 end\n",
      "Epoch #94 start\n",
      "Epoch #94 end\n",
      "Epoch #95 start\n",
      "Epoch #95 end\n",
      "Epoch #96 start\n",
      "Epoch #96 end\n",
      "Epoch #97 start\n",
      "Epoch #97 end\n",
      "Epoch #98 start\n",
      "Epoch #98 end\n",
      "Epoch #99 start\n",
      "Epoch #99 end\n"
     ]
    }
   ],
   "source": [
    "if Train_Word2vec:\n",
    "    phrases = []\n",
    "    for i in range(50):\n",
    "        print(i)\n",
    "        f = open(f\"streamspot/{i}.txt\")\n",
    "        data = f.read().split('\\n')\n",
    "\n",
    "        data = [line.split('\\t') for line in data]\n",
    "        df = pd.DataFrame (data, columns = ['actorID', 'actor_type','objectID','object','action','timestamp'])\n",
    "        df = df.dropna()\n",
    "        docs,labels,edges,mapp = prepare_graph(df)\n",
    "        phrases = phrases + docs\n",
    "        \n",
    "    word2vec = Word2Vec(sentences=phrases, vector_size=30, window=10, min_count=1, workers=8,epochs=100,callbacks=[saver,logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "p3TAi69zI1bO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "model = GCN(30,8).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Vn_pMyt5Jd-6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "class PositionalEncoder:\n",
    "\n",
    "    def __init__(self, d_model, max_len=100000):\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        self.pe = torch.zeros(max_len, d_model)\n",
    "        self.pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    def embed(self, x):\n",
    "        return x + self.pe[:x.size(0)]\n",
    "\n",
    "def infer(document):\n",
    "    if Train_Word2vec:                  ## added.\n",
    "        word_embeddings = [word2vec.wv[word] for word in document if word in  word2vec.wv]\n",
    "    else:\n",
    "        word_embeddings = [w2vmodel.wv[word] for word in document if word in  w2vmodel.wv]\n",
    "    \n",
    "    if not word_embeddings:\n",
    "        return np.zeros(20)\n",
    "\n",
    "    output_embedding = torch.tensor(word_embeddings, dtype=torch.float)\n",
    "    if len(document) < 100000:\n",
    "        output_embedding = encoder.embed(output_embedding)\n",
    "\n",
    "    output_embedding = output_embedding.detach().cpu().numpy()\n",
    "    return np.mean(output_embedding, axis=0)\n",
    "\n",
    "encoder = PositionalEncoder(30)\n",
    "w2vmodel = Word2Vec.load(\"trained_weights/streamspot/streamspot.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 689309,
     "status": "ok",
     "timestamp": 1673566932746,
     "user": {
      "displayName": "Mati Ur Rehman",
      "userId": "04281203290774044297"
     },
     "user_tz": 300
    },
    "id": "Gclj6HVL17lD",
    "outputId": "f60fadea-a7db-471f-defe-fee744f6ef25",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2143, device='cuda:0')\n",
      "tensor(0.7181, device='cuda:0')\n",
      "tensor(0.8576, device='cuda:0')\n",
      "tensor(0.9327, device='cuda:0')\n",
      "tensor(0.9367, device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8932, device='cuda:0')\n",
      "tensor(0.9618, device='cuda:0')\n",
      "tensor(0.9449, device='cuda:0')\n",
      "tensor(0.9622, device='cuda:0')\n",
      "tensor(0.9710, device='cuda:0')\n",
      "tensor(0.9760, device='cuda:0')\n",
      "tensor(0.9669, device='cuda:0')\n",
      "tensor(0.9722, device='cuda:0')\n",
      "tensor(0.9726, device='cuda:0')\n",
      "tensor(0.9771, device='cuda:0')\n",
      "tensor(0.9789, device='cuda:0')\n",
      "tensor(0.9738, device='cuda:0')\n",
      "tensor(0.9780, device='cuda:0')\n",
      "tensor(0.9778, device='cuda:0')\n",
      "tensor(0.9762, device='cuda:0')\n",
      "tensor(0.9688, device='cuda:0')\n",
      "tensor(0.9788, device='cuda:0')\n",
      "tensor(0.9824, device='cuda:0')\n",
      "tensor(0.9804, device='cuda:0')\n",
      "tensor(0.9810, device='cuda:0')\n",
      "tensor(0.9811, device='cuda:0')\n",
      "tensor(0.9819, device='cuda:0')\n",
      "tensor(0.9816, device='cuda:0')\n",
      "tensor(0.9795, device='cuda:0')\n",
      "tensor(0.9800, device='cuda:0')\n",
      "tensor(0.9830, device='cuda:0')\n",
      "tensor(0.9806, device='cuda:0')\n",
      "tensor(0.9821, device='cuda:0')\n",
      "tensor(0.9832, device='cuda:0')\n",
      "tensor(0.9839, device='cuda:0')\n",
      "tensor(0.9805, device='cuda:0')\n",
      "tensor(0.9835, device='cuda:0')\n",
      "tensor(0.9807, device='cuda:0')\n",
      "tensor(0.9812, device='cuda:0')\n",
      "tensor(0.9794, device='cuda:0')\n",
      "tensor(0.9833, device='cuda:0')\n",
      "tensor(0.9837, device='cuda:0')\n",
      "tensor(0.9830, device='cuda:0')\n",
      "tensor(0.9827, device='cuda:0')\n",
      "tensor(0.9825, device='cuda:0')\n",
      "tensor(0.9836, device='cuda:0')\n",
      "tensor(0.9808, device='cuda:0')\n",
      "tensor(0.9829, device='cuda:0')\n",
      "tensor(0.9828, device='cuda:0')\n",
      "tensor(0.9824, device='cuda:0')\n",
      "tensor(0.9832, device='cuda:0')\n",
      "tensor(0.9832, device='cuda:0')\n",
      "tensor(0.9829, device='cuda:0')\n",
      "tensor(0.9834, device='cuda:0')\n",
      "tensor(0.9827, device='cuda:0')\n",
      "tensor(0.9813, device='cuda:0')\n",
      "tensor(0.9831, device='cuda:0')\n",
      "tensor(0.9824, device='cuda:0')\n",
      "tensor(0.9832, device='cuda:0')\n",
      "tensor(0.9823, device='cuda:0')\n",
      "tensor(0.9831, device='cuda:0')\n",
      "tensor(0.9825, device='cuda:0')\n",
      "tensor(0.9836, device='cuda:0')\n",
      "tensor(0.9842, device='cuda:0')\n",
      "tensor(0.9840, device='cuda:0')\n",
      "tensor(0.9829, device='cuda:0')\n",
      "tensor(0.9818, device='cuda:0')\n",
      "tensor(0.9843, device='cuda:0')\n",
      "tensor(0.9839, device='cuda:0')\n",
      "tensor(0.9841, device='cuda:0')\n",
      "tensor(0.9840, device='cuda:0')\n",
      "tensor(0.9830, device='cuda:0')\n",
      "tensor(0.9844, device='cuda:0')\n",
      "tensor(0.9842, device='cuda:0')\n",
      "tensor(0.9804, device='cuda:0')\n",
      "tensor(0.9848, device='cuda:0')\n",
      "tensor(0.9836, device='cuda:0')\n",
      "tensor(0.9855, device='cuda:0')\n",
      "tensor(0.9850, device='cuda:0')\n",
      "tensor(0.9854, device='cuda:0')\n",
      "tensor(0.9822, device='cuda:0')\n",
      "tensor(0.9858, device='cuda:0')\n",
      "tensor(0.9838, device='cuda:0')\n",
      "tensor(0.9858, device='cuda:0')\n",
      "tensor(0.9855, device='cuda:0')\n",
      "tensor(0.9848, device='cuda:0')\n",
      "tensor(0.9853, device='cuda:0')\n",
      "tensor(0.9838, device='cuda:0')\n",
      "tensor(0.9856, device='cuda:0')\n",
      "tensor(0.9856, device='cuda:0')\n",
      "tensor(0.9823, device='cuda:0')\n",
      "tensor(0.9861, device='cuda:0')\n",
      "tensor(0.9857, device='cuda:0')\n",
      "tensor(0.9863, device='cuda:0')\n",
      "tensor(0.9857, device='cuda:0')\n",
      "tensor(0.9857, device='cuda:0')\n",
      "tensor(0.9853, device='cuda:0')\n",
      "tensor(0.9865, device='cuda:0')\n",
      "tensor(0.9860, device='cuda:0')\n",
      "tensor(0.9861, device='cuda:0')\n",
      "tensor(0.9846, device='cuda:0')\n",
      "tensor(0.9865, device='cuda:0')\n",
      "tensor(0.9875, device='cuda:0')\n",
      "tensor(0.9889, device='cuda:0')\n",
      "tensor(0.9899, device='cuda:0')\n",
      "tensor(0.9905, device='cuda:0')\n",
      "tensor(0.9885, device='cuda:0')\n",
      "tensor(0.9877, device='cuda:0')\n",
      "tensor(0.9880, device='cuda:0')\n",
      "tensor(0.9880, device='cuda:0')\n",
      "tensor(0.9900, device='cuda:0')\n",
      "tensor(0.9878, device='cuda:0')\n",
      "tensor(0.9878, device='cuda:0')\n",
      "tensor(0.9882, device='cuda:0')\n",
      "tensor(0.9888, device='cuda:0')\n",
      "tensor(0.9878, device='cuda:0')\n",
      "tensor(0.9887, device='cuda:0')\n",
      "tensor(0.9897, device='cuda:0')\n",
      "tensor(0.9877, device='cuda:0')\n",
      "tensor(0.9881, device='cuda:0')\n",
      "tensor(0.9885, device='cuda:0')\n",
      "tensor(0.9897, device='cuda:0')\n",
      "tensor(0.9881, device='cuda:0')\n",
      "tensor(0.9901, device='cuda:0')\n",
      "tensor(0.9891, device='cuda:0')\n",
      "tensor(0.9887, device='cuda:0')\n",
      "tensor(0.9890, device='cuda:0')\n",
      "tensor(0.9885, device='cuda:0')\n",
      "tensor(0.9875, device='cuda:0')\n",
      "tensor(0.9907, device='cuda:0')\n",
      "tensor(0.9897, device='cuda:0')\n",
      "tensor(0.9906, device='cuda:0')\n",
      "tensor(0.9894, device='cuda:0')\n",
      "tensor(0.9909, device='cuda:0')\n",
      "tensor(0.9894, device='cuda:0')\n",
      "tensor(0.9865, device='cuda:0')\n",
      "tensor(0.9911, device='cuda:0')\n",
      "tensor(0.9908, device='cuda:0')\n",
      "tensor(0.9866, device='cuda:0')\n",
      "tensor(0.9915, device='cuda:0')\n",
      "tensor(0.9893, device='cuda:0')\n",
      "tensor(0.9905, device='cuda:0')\n",
      "tensor(0.9908, device='cuda:0')\n",
      "tensor(0.9907, device='cuda:0')\n",
      "tensor(0.9893, device='cuda:0')\n",
      "tensor(0.9900, device='cuda:0')\n",
      "tensor(0.9902, device='cuda:0')\n",
      "tensor(0.9904, device='cuda:0')\n",
      "tensor(0.9893, device='cuda:0')\n",
      "tensor(0.9901, device='cuda:0')\n",
      "tensor(0.9891, device='cuda:0')\n",
      "tensor(0.9900, device='cuda:0')\n",
      "tensor(0.9911, device='cuda:0')\n",
      "tensor(0.9914, device='cuda:0')\n",
      "tensor(0.9901, device='cuda:0')\n",
      "tensor(0.9894, device='cuda:0')\n",
      "tensor(0.9904, device='cuda:0')\n",
      "tensor(0.9911, device='cuda:0')\n",
      "tensor(0.9906, device='cuda:0')\n",
      "tensor(0.9895, device='cuda:0')\n",
      "tensor(0.9908, device='cuda:0')\n",
      "tensor(0.9902, device='cuda:0')\n",
      "tensor(0.9906, device='cuda:0')\n",
      "tensor(0.9900, device='cuda:0')\n",
      "tensor(0.9900, device='cuda:0')\n",
      "tensor(0.9918, device='cuda:0')\n",
      "tensor(0.9899, device='cuda:0')\n",
      "tensor(0.9910, device='cuda:0')\n",
      "tensor(0.9916, device='cuda:0')\n",
      "tensor(0.9905, device='cuda:0')\n",
      "tensor(0.9899, device='cuda:0')\n",
      "tensor(0.9912, device='cuda:0')\n",
      "tensor(0.9917, device='cuda:0')\n",
      "tensor(0.9911, device='cuda:0')\n",
      "tensor(0.9908, device='cuda:0')\n",
      "tensor(0.9924, device='cuda:0')\n",
      "tensor(0.9890, device='cuda:0')\n",
      "tensor(0.9926, device='cuda:0')\n",
      "tensor(0.9911, device='cuda:0')\n",
      "tensor(0.9903, device='cuda:0')\n",
      "tensor(0.9924, device='cuda:0')\n",
      "tensor(0.9905, device='cuda:0')\n",
      "tensor(0.9911, device='cuda:0')\n",
      "tensor(0.9920, device='cuda:0')\n",
      "tensor(0.9914, device='cuda:0')\n",
      "tensor(0.9915, device='cuda:0')\n",
      "tensor(0.9909, device='cuda:0')\n",
      "tensor(0.9918, device='cuda:0')\n",
      "tensor(0.9915, device='cuda:0')\n",
      "tensor(0.9903, device='cuda:0')\n",
      "tensor(0.9909, device='cuda:0')\n",
      "tensor(0.9909, device='cuda:0')\n",
      "tensor(0.9911, device='cuda:0')\n",
      "tensor(0.9903, device='cuda:0')\n",
      "tensor(0.9924, device='cuda:0')\n",
      "tensor(0.9912, device='cuda:0')\n",
      "tensor(0.9920, device='cuda:0')\n",
      "tensor(0.9917, device='cuda:0')\n",
      "tensor(0.9912, device='cuda:0')\n",
      "tensor(0.9905, device='cuda:0')\n",
      "tensor(0.9824, device='cuda:0')\n",
      "tensor(0.9883, device='cuda:0')\n",
      "tensor(0.9875, device='cuda:0')\n",
      "tensor(0.9844, device='cuda:0')\n",
      "tensor(0.9895, device='cuda:0')\n",
      "tensor(0.9875, device='cuda:0')\n",
      "tensor(0.9897, device='cuda:0')\n",
      "tensor(0.9874, device='cuda:0')\n",
      "tensor(0.9888, device='cuda:0')\n",
      "tensor(0.9898, device='cuda:0')\n",
      "tensor(0.9895, device='cuda:0')\n",
      "tensor(0.9877, device='cuda:0')\n",
      "tensor(0.9885, device='cuda:0')\n",
      "tensor(0.9893, device='cuda:0')\n",
      "tensor(0.9889, device='cuda:0')\n",
      "tensor(0.9900, device='cuda:0')\n",
      "tensor(0.9838, device='cuda:0')\n",
      "tensor(0.9858, device='cuda:0')\n",
      "tensor(0.9887, device='cuda:0')\n",
      "tensor(0.9896, device='cuda:0')\n",
      "tensor(0.9878, device='cuda:0')\n",
      "tensor(0.9899, device='cuda:0')\n",
      "tensor(0.9906, device='cuda:0')\n",
      "tensor(0.9899, device='cuda:0')\n",
      "tensor(0.9902, device='cuda:0')\n",
      "tensor(0.9901, device='cuda:0')\n",
      "tensor(0.9896, device='cuda:0')\n",
      "tensor(0.9913, device='cuda:0')\n",
      "tensor(0.9909, device='cuda:0')\n",
      "tensor(0.9918, device='cuda:0')\n",
      "tensor(0.9915, device='cuda:0')\n",
      "tensor(0.9913, device='cuda:0')\n",
      "tensor(0.9907, device='cuda:0')\n",
      "tensor(0.9865, device='cuda:0')\n",
      "tensor(0.9798, device='cuda:0')\n",
      "tensor(0.9921, device='cuda:0')\n",
      "tensor(0.9921, device='cuda:0')\n",
      "tensor(0.9908, device='cuda:0')\n",
      "tensor(0.9907, device='cuda:0')\n",
      "tensor(0.9910, device='cuda:0')\n",
      "tensor(0.9921, device='cuda:0')\n",
      "tensor(0.9901, device='cuda:0')\n",
      "tensor(0.9923, device='cuda:0')\n",
      "tensor(0.9864, device='cuda:0')\n",
      "tensor(0.9916, device='cuda:0')\n",
      "tensor(0.9923, device='cuda:0')\n",
      "tensor(0.9917, device='cuda:0')\n",
      "tensor(0.9922, device='cuda:0')\n",
      "tensor(0.9912, device='cuda:0')\n",
      "tensor(0.9902, device='cuda:0')\n",
      "tensor(0.9903, device='cuda:0')\n",
      "tensor(0.9912, device='cuda:0')\n",
      "tensor(0.9916, device='cuda:0')\n",
      "tensor(0.9914, device='cuda:0')\n",
      "tensor(0.9924, device='cuda:0')\n",
      "tensor(0.9890, device='cuda:0')\n",
      "tensor(0.9890, device='cuda:0')\n",
      "tensor(0.9921, device='cuda:0')\n",
      "tensor(0.9919, device='cuda:0')\n",
      "tensor(0.9925, device='cuda:0')\n",
      "tensor(0.9889, device='cuda:0')\n",
      "tensor(0.9921, device='cuda:0')\n",
      "tensor(0.9918, device='cuda:0')\n",
      "tensor(0.9915, device='cuda:0')\n",
      "tensor(0.9919, device='cuda:0')\n",
      "tensor(0.9922, device='cuda:0')\n",
      "tensor(0.9924, device='cuda:0')\n",
      "tensor(0.9917, device='cuda:0')\n",
      "tensor(0.9888, device='cuda:0')\n",
      "tensor(0.9910, device='cuda:0')\n",
      "tensor(0.9924, device='cuda:0')\n",
      "tensor(0.9914, device='cuda:0')\n",
      "tensor(0.9915, device='cuda:0')\n",
      "tensor(0.9929, device='cuda:0')\n",
      "tensor(0.9924, device='cuda:0')\n",
      "tensor(0.9925, device='cuda:0')\n",
      "tensor(0.9890, device='cuda:0')\n",
      "tensor(0.9903, device='cuda:0')\n",
      "tensor(0.9922, device='cuda:0')\n",
      "tensor(0.9921, device='cuda:0')\n",
      "tensor(0.9927, device='cuda:0')\n",
      "tensor(0.9932, device='cuda:0')\n",
      "tensor(0.9916, device='cuda:0')\n",
      "tensor(0.9891, device='cuda:0')\n",
      "tensor(0.9921, device='cuda:0')\n",
      "tensor(0.9921, device='cuda:0')\n",
      "tensor(0.9921, device='cuda:0')\n",
      "tensor(0.9917, device='cuda:0')\n",
      "tensor(0.9924, device='cuda:0')\n",
      "tensor(0.9914, device='cuda:0')\n",
      "tensor(0.9931, device='cuda:0')\n",
      "tensor(0.9824, device='cuda:0')\n",
      "tensor(0.9930, device='cuda:0')\n",
      "tensor(0.9922, device='cuda:0')\n",
      "tensor(0.9931, device='cuda:0')\n",
      "tensor(0.9927, device='cuda:0')\n",
      "tensor(0.9925, device='cuda:0')\n",
      "tensor(0.9920, device='cuda:0')\n",
      "tensor(0.9919, device='cuda:0')\n",
      "tensor(0.9930, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric import utils\n",
    "\n",
    "if Train_Gnn:\n",
    "    for i in range(300):                        ## trained till 300.\n",
    "    # for i in range(400):\n",
    "        f = open(f\"streamspot/{i}.txt\")\n",
    "        \n",
    "        data = f.read().split('\\n')\n",
    "\n",
    "        data = [line.split('\\t') for line in data]\n",
    "        df = pd.DataFrame (data, columns = ['actorID', 'actor_type','objectID','object','action','timestamp'])\n",
    "        df = df.dropna()\n",
    "        phrases,labels,edges,mapp = prepare_graph(df)\n",
    "\n",
    "        criterion = CrossEntropyLoss()\n",
    "\n",
    "        nodes = [infer(x) for x in phrases]\n",
    "        nodes = np.array(nodes)  \n",
    "\n",
    "        graph = Data(x=torch.tensor(nodes,dtype=torch.float).to(device),y=torch.tensor(labels,dtype=torch.long).to(device), edge_index=torch.tensor(edges,dtype=torch.long).to(device))\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad() \n",
    "        out = model(graph.x, graph.edge_index) \n",
    "        loss = criterion(out, graph.y) \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "\n",
    "        _ , indices = out.sort(dim=1,descending=True)\n",
    "        pred = indices[:,0]\n",
    "        cond = pred == graph.y\n",
    "\n",
    "        print(cond.sum() / len(graph.y))\n",
    "\n",
    "        torch.save(model.state_dict(), f'trained_weights/streamspot/lstreamspot.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 1.1112384007331884\n",
      "97 1.1002722323049001\n",
      "78 0.8956252152945229\n",
      "97 1.097285067873303\n",
      "92 1.046048891415577\n",
      "84 0.9573740597219056\n",
      "86 0.9921550530687587\n",
      "97 1.1167395809348377\n",
      "90 1.0359116022099446\n",
      "98 1.121666475907062\n",
      "84 0.9610983981693364\n",
      "89 1.0350040702407257\n",
      "97 1.115840331301047\n",
      "90 1.0251737099897482\n",
      "87 1.0069444444444444\n",
      "100 1.1328877308258751\n",
      "91 1.0476629058254663\n",
      "97 1.101396616327921\n",
      "98 1.104349785891368\n",
      "93 1.0556186152099887\n",
      "94 1.0670904756499036\n",
      "83 0.9574345368554621\n",
      "84 0.9535702122828924\n",
      "83 0.9492223238792316\n",
      "97 1.0875658706132976\n",
      "90 1.0184451737014826\n",
      "87 0.9953094611600504\n",
      "91 1.0280162675101672\n",
      "87 0.9879627526686351\n",
      "86 0.9708737864077669\n",
      "97 1.0919734323989643\n",
      "77 0.8752983971808572\n",
      "84 0.9496890898812888\n",
      "84 0.9484023935869933\n",
      "74 0.8413871517907903\n",
      "90 1.0182147301730966\n",
      "89 0.9997753313862054\n",
      "79 0.9002849002849003\n",
      "84 0.9630818619582664\n",
      "105 1.1651131824234355\n",
      "80 0.9092975676290065\n",
      "83 0.9400838147015518\n",
      "91 1.0218978102189782\n",
      "89 1.0037216645990752\n",
      "109 1.2308039747064137\n",
      "92 1.028507546115148\n",
      "88 0.9931159011398263\n",
      "96 1.076957594794705\n",
      "82 0.9250902527075812\n",
      "75 0.8566533409480296\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'trained_weights/streamspot/lstreamspot.pth', map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "for i in range(400,450):                            ## validating 400 to 450, while trained till 300?\n",
    "    f = open(f\"streamspot/{i}.txt\")\n",
    "    data = f.read().split('\\n')\n",
    "\n",
    "    data = [line.split('\\t') for line in data]\n",
    "    df = pd.DataFrame (data, columns = ['actorID', 'actor_type','objectID','object','action','timestamp'])\n",
    "    df = df.dropna()\n",
    "    \n",
    "    phrases,labels,edges,mapp = prepare_graph(df)\n",
    "\n",
    "    nodes = [infer(x) for x in phrases]\n",
    "    nodes = np.array(nodes)\n",
    "    \n",
    "    graph = Data(x=torch.tensor(nodes,dtype=torch.float).to(device),y=torch.tensor(labels,dtype=torch.long).to(device), edge_index=torch.tensor(edges,dtype=torch.long).to(device))\n",
    "    graph.n_id = torch.arange(graph.num_nodes)\n",
    "    flag = torch.tensor([True]*graph.num_nodes, dtype=torch.bool)\n",
    "\n",
    "    out = model(graph.x, graph.edge_index)\n",
    "\n",
    "    sorted, indices = out.sort(dim=1,descending=True)\n",
    "    conf = (sorted[:,0] - sorted[:,1]) / sorted[:,0]\n",
    "    conf = (conf - conf.min()) / conf.max()\n",
    "\n",
    "    pred = indices[:,0]\n",
    "    cond = ~(pred == graph.y)\n",
    "    \n",
    "    print(cond.sum().item(), (cond.sum().item() / len(cond))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "thresh = 200\n",
    "correct_benign = 0\n",
    "correct_attack = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (conv1): SAGEConv(30, 32, aggr=mean)\n",
       "  (conv2): SAGEConv(32, 8, aggr=mean)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(f'trained_weights/streamspot/lstreamspot.pth',map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 1.0762232331077333\n",
      "74 0.840718018632129\n",
      "91 1.023737203284959\n",
      "89 1.0041746586934446\n",
      "88 0.9994321408290744\n",
      "91 1.0306943028655566\n",
      "94 1.0576057605760576\n",
      "83 0.9344742175185768\n",
      "93 1.0476512335248396\n",
      "89 0.9951917700995192\n",
      "89 1.0098717803245207\n",
      "82 0.9257168661097314\n",
      "79 0.8914466260437826\n",
      "98 1.1009998876530727\n",
      "92 1.0254123941150244\n",
      "75 0.853825136612022\n",
      "82 0.9272871197557391\n",
      "81 0.9177430319510537\n",
      "100 1.1189437171310284\n",
      "87 0.9889735136978516\n",
      "80 0.9012053621719048\n",
      "97 1.0923423423423424\n",
      "78 0.8843537414965987\n",
      "84 0.9509792822370655\n",
      "89 0.9998876530726885\n",
      "91 1.0171007041466413\n",
      "77 0.8740068104426788\n",
      "84 0.9572649572649573\n",
      "87 0.9752269924896312\n",
      "90 1.0102143899427545\n",
      "94 1.0532212885154062\n",
      "75 0.8461191335740073\n",
      "88 0.9915492957746479\n",
      "88 0.9916610322289835\n",
      "100 1.1273957158962795\n",
      "80 0.9136592051164915\n",
      "83 0.9380650994575045\n",
      "78 0.8845543207076435\n",
      "82 0.929810636126545\n",
      "82 0.9288627095604893\n",
      "94 1.0544026920919798\n",
      "92 1.0349870626617166\n",
      "96 1.079865016872891\n",
      "78 0.88515660463005\n",
      "82 0.9306548632391329\n",
      "99 1.1136107986501687\n",
      "88 1.0017074558907229\n",
      "87 0.9862827343838566\n",
      "86 0.9689049121225778\n",
      "81 0.9039169735520589\n",
      "80 0.8880994671403196\n",
      "87 0.9702241552358649\n",
      "81 0.9018036072144289\n",
      "86 0.9567248859717433\n",
      "75 0.8362136247073252\n",
      "82 0.9150764423613437\n",
      "79 0.8810081409613025\n",
      "85 0.945179584120983\n",
      "84 0.9384426321081444\n",
      "85 0.9480258755297791\n",
      "99 1.083743842364532\n",
      "82 0.9156895589056394\n",
      "77 0.8571746632528108\n",
      "80 0.8917623453349682\n",
      "76 0.8455718736092568\n",
      "76 0.8483089630539122\n",
      "81 0.9020044543429844\n",
      "74 0.8239616969157109\n",
      "75 0.8331481892912687\n",
      "75 0.8356545961002786\n",
      "90 0.9965673790277931\n",
      "79 0.8787541713014461\n",
      "77 0.8582255907267053\n",
      "85 0.947603121516165\n",
      "81 0.9034128931519072\n",
      "78 0.8667629736637403\n",
      "87 0.9633484663935333\n",
      "83 0.9261325596964962\n",
      "81 0.9008007117437723\n",
      "86 0.9421560035056966\n",
      "68 0.760796598791676\n",
      "74 0.8258006918870662\n",
      "84 0.9380234505862647\n",
      "86 0.9547069271758437\n",
      "77 0.8585126546995206\n",
      "65 0.7239113487025282\n",
      "86 0.956193017567267\n",
      "80 0.8913649025069639\n",
      "77 0.8562215056154787\n",
      "77 0.859087359143144\n",
      "82 0.9107063527321191\n",
      "89 0.9936362621413419\n",
      "70 0.7836990595611284\n",
      "94 1.034217185608978\n",
      "78 0.8685968819599109\n",
      "83 0.9236590251502338\n",
      "81 0.9020044543429844\n",
      "98 1.0742080455990355\n",
      "77 0.8583212573849068\n",
      "77 0.8743045304871124\n",
      "84 0.9258238730298688\n",
      "84 0.933852140077821\n",
      "100 1.0771219302024988\n",
      "83 0.924791086350975\n",
      "81 0.9021049114600734\n",
      "76 0.8468910184978827\n",
      "76 0.8477412158393753\n",
      "93 1.0189547496439137\n",
      "78 0.8707300736771602\n",
      "81 0.9022053909556694\n",
      "93 1.0293303818483674\n",
      "78 0.869468286701594\n",
      "92 1.0223358150905657\n",
      "79 0.8814996652532917\n",
      "85 0.9460211463550361\n",
      "71 0.7913508693713777\n",
      "88 0.9743135518157661\n",
      "68 0.761478163493841\n",
      "84 0.9350996326394301\n",
      "91 1.0105496946141033\n",
      "72 0.8041997095945493\n",
      "79 0.8810081409613025\n",
      "76 0.8471742280682198\n",
      "90 0.9998889012331964\n",
      "108 1.1830430496220834\n",
      "76 0.8480249944208882\n",
      "85 0.9482373940205265\n",
      "93 1.027170311464546\n",
      "75 0.8330556481172943\n",
      "79 0.8799287146357764\n",
      "79 0.8778753194799421\n",
      "70 0.7785563341118896\n",
      "77 0.857843137254902\n",
      "85 0.9494024349380096\n",
      "71 0.7927646270656542\n",
      "78 0.8690807799442898\n",
      "83 0.9223247027447494\n",
      "80 0.8899766381132496\n",
      "83 0.9199733983595655\n",
      "84 0.9388621884430536\n",
      "87 0.9705488621151271\n",
      "83 0.9242761692650333\n",
      "83 0.9223247027447494\n",
      "87 0.9673115410273516\n",
      "77 0.8594709230940953\n",
      "82 0.9118203046814188\n",
      "77 0.859662833538015\n",
      "84 0.9413874257536703\n",
      "74 0.8212185107091332\n",
      "76 0.8471742280682198\n"
     ]
    }
   ],
   "source": [
    "for i in range(450,600):\n",
    "    f = open(f\"streamspot/{i}.txt\")\n",
    "    data = f.read().split('\\n')\n",
    "\n",
    "    data = [line.split('\\t') for line in data]\n",
    "    df = pd.DataFrame (data, columns = ['actorID', 'actor_type','objectID','object','action','timestamp'])\n",
    "    df = df.dropna()\n",
    "    \n",
    "    phrases,labels,edges,mapp = prepare_graph(df)\n",
    "\n",
    "    nodes = [infer(x) for x in phrases]\n",
    "    nodes = np.array(nodes)\n",
    "    \n",
    "    graph = Data(x=torch.tensor(nodes,dtype=torch.float).to(device),y=torch.tensor(labels,dtype=torch.long).to(device), edge_index=torch.tensor(edges,dtype=torch.long).to(device))\n",
    "    graph.n_id = torch.arange(graph.num_nodes)\n",
    "    flag = torch.tensor([True]*graph.num_nodes, dtype=torch.bool)\n",
    "\n",
    "    out = model(graph.x, graph.edge_index)\n",
    "\n",
    "    sorted, indices = out.sort(dim=1,descending=True)\n",
    "    conf = (sorted[:,0] - sorted[:,1]) / sorted[:,0]\n",
    "    conf = (conf - conf.min()) / conf.max()\n",
    "\n",
    "    pred = indices[:,0]\n",
    "    cond = ~(pred == graph.y)\n",
    "\n",
    "    if cond.sum() <= thresh:\n",
    "         correct_benign = correct_benign + 1\n",
    "    \n",
    "    print(cond.sum().item(), (cond.sum().item() / len(cond))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1072 11.995076647644623\n",
      "1076 12.031756681203177\n",
      "1070 11.963327370304116\n",
      "1076 12.026377556722924\n",
      "1073 12.000894754501735\n",
      "1075 12.017887087758524\n",
      "1073 11.999552672780139\n",
      "1073 11.995528228060369\n",
      "1072 11.987028961198703\n",
      "1073 12.008953553441522\n",
      "1074 12.013422818791947\n",
      "1075 12.023263617045073\n",
      "1075 12.025953686094642\n",
      "1076 12.02503352704515\n",
      "1074 12.004023695093327\n",
      "1071 11.98791134989926\n",
      "1071 11.973169368362214\n",
      "1074 12.006707657909446\n",
      "1071 11.975847031197585\n",
      "1073 11.995528228060369\n",
      "1072 11.984348798211292\n",
      "1075 12.023263617045073\n",
      "1076 12.031756681203177\n",
      "1072 11.985688729874775\n",
      "1074 12.001340931947704\n",
      "1075 12.025953686094642\n",
      "1070 11.96600313129054\n",
      "1076 12.033102214269737\n",
      "1073 12.008953553441522\n",
      "1078 12.054120541205412\n",
      "1073 12.002237136465324\n",
      "48 0.6111535523300229\n",
      "1073 12.000894754501735\n",
      "48 0.6066734074823054\n",
      "47 0.5941094678296044\n",
      "1072 11.984348798211292\n",
      "1076 12.034448048316742\n",
      "1071 11.973169368362214\n",
      "1073 11.998210891199822\n",
      "1076 12.02503352704515\n",
      "1073 12.008953553441522\n",
      "1071 11.978525891958395\n",
      "1072 11.984348798211292\n",
      "1072 11.987028961198703\n",
      "1075 12.016543706684551\n",
      "1075 12.016543706684551\n",
      "1071 11.974508050089446\n",
      "1072 11.987028961198703\n",
      "1073 12.000894754501735\n",
      "1075 12.020574751202059\n",
      "1073 11.995528228060369\n",
      "1071 11.981205951448707\n",
      "1072 11.978992066152642\n",
      "1072 11.985688729874775\n",
      "1074 12.008050089445439\n",
      "1073 12.002237136465324\n",
      "1073 11.995528228060369\n",
      "1076 12.034448048316742\n",
      "1074 12.008050089445439\n",
      "1074 12.00939282120094\n",
      "1075 12.01923076923077\n",
      "1074 12.017455521987245\n",
      "1075 12.012515364845235\n",
      "1071 11.975847031197585\n",
      "1069 11.950810508664059\n",
      "1075 12.025953686094642\n",
      "1071 11.967817633255112\n",
      "1072 11.987028961198703\n",
      "1072 11.987028961198703\n",
      "1073 11.990166499050172\n",
      "1072 11.987028961198703\n",
      "1074 12.014766752433157\n",
      "1076 12.034448048316742\n",
      "1074 12.00939282120094\n",
      "1072 11.984348798211292\n",
      "1074 12.010735853276671\n",
      "1075 12.013857845328566\n",
      "1072 11.993734616245245\n",
      "1074 12.00939282120094\n",
      "1076 12.031756681203177\n",
      "1074 12.006707657909446\n",
      "1073 12.000894754501735\n",
      "1074 12.004023695093327\n",
      "51 0.6447534766118838\n",
      "1076 12.034448048316742\n",
      "1076 12.034448048316742\n",
      "1075 12.012515364845235\n",
      "1076 12.031756681203177\n",
      "48 0.606826801517067\n",
      "1074 12.006707657909446\n",
      "1074 12.010735853276671\n",
      "1076 12.0304114490161\n",
      "1073 11.994187346300023\n",
      "1071 11.971830985915492\n",
      "1072 11.991051454138702\n",
      "1071 11.97986577181208\n",
      "1073 11.995528228060369\n",
      "1073 11.996869409660107\n",
      "1074 12.00939282120094\n",
      "1076 12.037140619756125\n"
     ]
    }
   ],
   "source": [
    "for i in range(300,400):\n",
    "    f = open(f\"streamspot/{i}.txt\")\n",
    "    data = f.read().split('\\n')\n",
    "\n",
    "    data = [line.split('\\t') for line in data]\n",
    "    df = pd.DataFrame (data, columns = ['actorID', 'actor_type','objectID','object','action','timestamp'])\n",
    "    df = df.dropna()\n",
    "    \n",
    "    phrases,labels,edges,mapp = prepare_graph(df)\n",
    "  \n",
    "    nodes = [infer(x) for x in phrases]\n",
    "    nodes = np.array(nodes)\n",
    "    \n",
    "    graph = Data(x=torch.tensor(nodes,dtype=torch.float).to(device),y=torch.tensor(labels,dtype=torch.long).to(device), edge_index=torch.tensor(edges,dtype=torch.long).to(device))\n",
    "    graph.n_id = torch.arange(graph.num_nodes)\n",
    "    flag = torch.tensor([True]*graph.num_nodes, dtype=torch.bool)\n",
    "\n",
    "    out = model(graph.x, graph.edge_index)\n",
    "\n",
    "    sorted, indices = out.sort(dim=1,descending=True)\n",
    "    conf = (sorted[:,0] - sorted[:,1]) / sorted[:,0]\n",
    "    conf = (conf - conf.min()) / conf.max()\n",
    "\n",
    "    pred = indices[:,0]\n",
    "    cond = ~(pred == graph.y)\n",
    "\n",
    "    if cond.sum() > thresh:\n",
    "         correct_attack = correct_attack + 1\n",
    "            \n",
    "    print(cond.sum().item(), (cond.sum().item() / len(cond))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of True Positives: 95\n",
      "Number of False Positives: 0\n",
      "Number of False Negatives: 5\n",
      "Number of True Negatives: 150\n",
      "\n",
      "Precision: 1.0\n",
      "Recall: 0.95\n",
      "Fscore: 0.9743589743589743\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TOTAL_ATTACKS = 100\n",
    "TOTAL_BENIGN = 150\n",
    "\n",
    "def calculate_metrics(correct_attack, correct_benign):\n",
    "    TP = correct_attack\n",
    "    FP = TOTAL_BENIGN - correct_benign\n",
    "    TN = correct_benign\n",
    "    FN = TOTAL_ATTACKS - correct_attack\n",
    "\n",
    "    FPR = FP / (FP + TN) if (FP + TN) > 0 else 0\n",
    "    TPR = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "    print(f\"Number of True Positives: {TP}\")\n",
    "    print(f\"Number of False Positives: {FP}\")\n",
    "    print(f\"Number of False Negatives: {FN}\")\n",
    "    print(f\"Number of True Negatives: {TN}\\n\")\n",
    "\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "\n",
    "    fscore = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    print(f\"Fscore: {fscore}\\n\")\n",
    "\n",
    "calculate_metrics(correct_attack, correct_benign)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "flash_amaan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
