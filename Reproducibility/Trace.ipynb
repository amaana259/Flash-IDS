{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flash Evaluation on DARPA E3 Trace Dataset: \n",
    "\n",
    "This notebook is specifically designed for the evaluation of Flash on the DARPA E3 Trace dataset. Notably, the Trace dataset is characterized as a node-level dataset. In our analysis, Flash is configured to operate in a node-level setting to aptly assess this dataset. A key aspect to note is that the Trace dataset lacks certain essential node attributes for specific node types. This limitation means that Flash cannot be operated in a decoupled mode with offline GNN embeddings for this dataset. Consequently, we employ an online GNN coupled with word2vec semantic embeddings to achieve effective evaluation results for this dataset.\n",
    "\n",
    "## Dataset Access: \n",
    "- Access the Trace dataset via the following link: [Trace Dataset](https://drive.google.com/drive/folders/1QlbUFWAGq3Hpl8wVdzOdIoZLFxkII4EK).\n",
    "- The dataset files will be downloaded automatically by the script.\n",
    "\n",
    "## Data Parsing and Execution:\n",
    "- The script is designed to automatically parse the downloaded data files.\n",
    "- Execute all cells within this notebook to obtain the evaluation results.\n",
    "\n",
    "## Model Training and Execution Flexibility:\n",
    "- The notebook is configured to use pre-trained model weights by default.\n",
    "- It also provides the option to set parameters for independently training Graph Neural Networks (GNNs) and word2vec models.\n",
    "- These newly trained models can then be utilized for a comprehensive evaluation of the dataset.\n",
    "\n",
    "Adhere to these steps for a detailed and effective analysis of the Trace dataset using Flash.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "F1op-CbyLuN4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tpiuser2/anaconda3/envs/flash_amaan/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/tpiuser2/anaconda3/envs/flash_amaan/lib/python3.8/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "warnings.filterwarnings('ignore')\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "import multiprocessing\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1GG1aUnPjjzzdbxznVTN8X6oVfA-K4oIV\n",
      "From (redirected): https://drive.google.com/uc?id=1GG1aUnPjjzzdbxznVTN8X6oVfA-K4oIV&confirm=t&uuid=067d4294-50bb-4203-b21a-f0a48a4077ef\n",
      "To: /home/tpiuser2/prov_project/amaan_flash/Flash-IDS-main/ta1-trace-e3-official-1.json.tar.gz\n",
      "100%|██████████| 1.28G/1.28G [11:22<00:00, 1.88MB/s]\n"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "urls = [\"https://drive.google.com/file/d/1GG1aUnPjjzzdbxznVTN8X6oVfA-K4oIV/view?usp=drive_link\"]\n",
    "for url in urls:\n",
    "    gdown.download(url, quiet=False, use_cookies=False, fuzzy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train = False\n",
    "Train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nM7KaeCbA_mQ"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import gzip\n",
    "from sklearn.manifold import TSNE\n",
    "import json\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_uuid(line):\n",
    "    pattern_uuid = re.compile(r'uuid\\\":\\\"(.*?)\\\"')\n",
    "    return pattern_uuid.findall(line)\n",
    "\n",
    "def extract_subject_type(line):\n",
    "    pattern_type = re.compile(r'type\\\":\\\"(.*?)\\\"')\n",
    "    return pattern_type.findall(line)\n",
    "\n",
    "def show(file_path):\n",
    "    print(f\"Processing {file_path}\")\n",
    "\n",
    "def extract_edge_info(line):\n",
    "    pattern_src = re.compile(r'subject\\\":{\\\"com.bbn.tc.schema.avro.cdm18.UUID\\\":\\\"(.*?)\\\"}')\n",
    "    pattern_dst1 = re.compile(r'predicateObject\\\":{\\\"com.bbn.tc.schema.avro.cdm18.UUID\\\":\\\"(.*?)\\\"}')\n",
    "    pattern_dst2 = re.compile(r'predicateObject2\\\":{\\\"com.bbn.tc.schema.avro.cdm18.UUID\\\":\\\"(.*?)\\\"}')\n",
    "    pattern_type = re.compile(r'type\\\":\\\"(.*?)\\\"')\n",
    "    pattern_time = re.compile(r'timestampNanos\\\":(.*?),')\n",
    "\n",
    "    edge_type = extract_subject_type(line)[0]\n",
    "    timestamp = pattern_time.findall(line)[0]\n",
    "    src_id = pattern_src.findall(line)\n",
    "\n",
    "    if len(src_id) == 0:\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    src_id = src_id[0]\n",
    "    dst_id1 = pattern_dst1.findall(line)\n",
    "    dst_id2 = pattern_dst2.findall(line)\n",
    "\n",
    "    if len(dst_id1) > 0 and dst_id1[0] != 'null':\n",
    "        dst_id1 = dst_id1[0]\n",
    "    else:\n",
    "        dst_id1 = None\n",
    "\n",
    "    if len(dst_id2) > 0 and dst_id2[0] != 'null':\n",
    "        dst_id2 = dst_id2[0]\n",
    "    else:\n",
    "        dst_id2 = None\n",
    "\n",
    "    return src_id, edge_type, timestamp, dst_id1, dst_id2\n",
    "\n",
    "def process_data(file_path):\n",
    "    id_nodetype_map = {}\n",
    "    notice_num = 1000000\n",
    "    for i in range(100):\n",
    "        now_path = file_path + '.' + str(i)\n",
    "        if i == 0:\n",
    "            now_path = file_path\n",
    "        if not os.path.exists(now_path):\n",
    "            break\n",
    "\n",
    "        with open(now_path, 'r') as f:\n",
    "            show(now_path)\n",
    "            cnt = 0\n",
    "            for line in f:\n",
    "                cnt += 1\n",
    "                if cnt % notice_num == 0:\n",
    "                    print(cnt)\n",
    "\n",
    "                if 'com.bbn.tc.schema.avro.cdm18.Event' in line or 'com.bbn.tc.schema.avro.cdm18.Host' in line:\n",
    "                    continue\n",
    "\n",
    "                if 'com.bbn.tc.schema.avro.cdm18.TimeMarker' in line or 'com.bbn.tc.schema.avro.cdm18.StartMarker' in line:\n",
    "                    continue\n",
    "\n",
    "                if 'com.bbn.tc.schema.avro.cdm18.UnitDependency' in line or 'com.bbn.tc.schema.avro.cdm18.EndMarker' in line:\n",
    "                    continue\n",
    "\n",
    "                uuid = extract_uuid(line)[0]\n",
    "                subject_type = extract_subject_type(line)\n",
    "\n",
    "                if len(subject_type) < 1:\n",
    "                    if 'com.bbn.tc.schema.avro.cdm18.MemoryObject' in line:\n",
    "                        id_nodetype_map[uuid] = 'MemoryObject'\n",
    "                        continue\n",
    "                    if 'com.bbn.tc.schema.avro.cdm18.NetFlowObject' in line:\n",
    "                        id_nodetype_map[uuid] = 'NetFlowObject'\n",
    "                        continue\n",
    "                    if 'com.bbn.tc.schema.avro.cdm18.UnnamedPipeObject' in line:\n",
    "                        id_nodetype_map[uuid] = 'UnnamedPipeObject'\n",
    "                        continue\n",
    "\n",
    "                id_nodetype_map[uuid] = subject_type[0]\n",
    "\n",
    "    return id_nodetype_map\n",
    "\n",
    "def process_edges(file_path, id_nodetype_map):\n",
    "    notice_num = 1000000\n",
    "    not_in_cnt = 0\n",
    "\n",
    "    for i in range(100):\n",
    "        now_path = file_path + '.' + str(i)\n",
    "        if i == 0:\n",
    "            now_path = file_path\n",
    "        if not os.path.exists(now_path):\n",
    "            break\n",
    "\n",
    "        with open(now_path, 'r') as f, open(now_path+'.txt', 'w') as fw:\n",
    "            cnt = 0\n",
    "            for line in f:\n",
    "                cnt += 1\n",
    "                if cnt % notice_num == 0:\n",
    "                    print(cnt)\n",
    "\n",
    "                if 'com.bbn.tc.schema.avro.cdm18.Event' in line:\n",
    "                    src_id, edge_type, timestamp, dst_id1, dst_id2 = extract_edge_info(line)\n",
    "\n",
    "                    if src_id is None or src_id not in id_nodetype_map:\n",
    "                        not_in_cnt += 1\n",
    "                        continue\n",
    "\n",
    "                    src_type = id_nodetype_map[src_id]\n",
    "\n",
    "                    if dst_id1 is not None and dst_id1 in id_nodetype_map:\n",
    "                        dst_type1 = id_nodetype_map[dst_id1]\n",
    "                        this_edge1 = f\"{src_id}\\t{src_type}\\t{dst_id1}\\t{dst_type1}\\t{edge_type}\\t{timestamp}\\n\"\n",
    "                        fw.write(this_edge1)\n",
    "\n",
    "                    if dst_id2 is not None and dst_id2 in id_nodetype_map:\n",
    "                        dst_type2 = id_nodetype_map[dst_id2]\n",
    "                        this_edge2 = f\"{src_id}\\t{src_type}\\t{dst_id2}\\t{dst_type2}\\t{edge_type}\\t{timestamp}\\n\"\n",
    "                        fw.write(this_edge2)\n",
    "\n",
    "def run_data_processing():\n",
    "    os.system('tar -zxvf ta1-trace-e3-official-1.json.tar.gz')\n",
    "\n",
    "    path_list = ['ta1-trace-e3-official-1.json']\n",
    "\n",
    "    for path in path_list:\n",
    "        id_nodetype_map = process_data(path)\n",
    "\n",
    "        process_edges(path, id_nodetype_map)\n",
    "\n",
    "    os.system('cp ta1-trace-e3-official-1.json.txt trace_train.txt')\n",
    "    os.system('cp ta1-trace-e3-official-1.json.4.txt trace_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ta1-trace-e3-official-1.json\n",
      "ta1-trace-e3-official-1.json.1\n",
      "ta1-trace-e3-official-1.json.2\n",
      "ta1-trace-e3-official-1.json.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ta1-trace-e3-official-1.json.4\n",
      "ta1-trace-e3-official-1.json.5\n",
      "ta1-trace-e3-official-1.json.6\n",
      "Processing ta1-trace-e3-official-1.json\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "Processing ta1-trace-e3-official-1.json.1\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "Processing ta1-trace-e3-official-1.json.2\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "Processing ta1-trace-e3-official-1.json.3\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "Processing ta1-trace-e3-official-1.json.4\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "Processing ta1-trace-e3-official-1.json.5\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "Processing ta1-trace-e3-official-1.json.6\n",
      "1000000\n",
      "2000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "1000000\n",
      "2000000\n"
     ]
    }
   ],
   "source": [
    "run_data_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_node_properties(nodes, node_id, properties):\n",
    "    if node_id not in nodes:\n",
    "        nodes[node_id] = []\n",
    "    nodes[node_id].extend(properties)\n",
    "\n",
    "def update_edge_index(edges, edge_index, index):\n",
    "    for src_id, dst_id in edges:\n",
    "        src = index[src_id]\n",
    "        dst = index[dst_id]\n",
    "        edge_index[0].append(src)\n",
    "        edge_index[1].append(dst)\n",
    "\n",
    "def prepare_graph(df):\n",
    "    nodes, labels, edges = {}, {}, []\n",
    "    dummies = {\"SUBJECT_PROCESS\":0, \"MemoryObject\":1, \"FILE_OBJECT_CHAR\":2, \"FILE_OBJECT_FILE\":3,\n",
    "               \"FILE_OBJECT_DIR\":4, \"SUBJECT_UNIT\":5, \"UnnamedPipeObject\":6, \"FILE_OBJECT_UNIX_SOCKET\":7, \n",
    "               \"SRCSINK_UNKNOWN\":8, \"FILE_OBJECT_LINK\":9, \"NetFlowObject\":10, \"FILE_OBJECT_BLOCK\":11}\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        action = row[\"action\"]\n",
    "        properties = [row['exec'], action] + ([row['path']] if row['path'] else [])\n",
    "        \n",
    "        actor_id = row[\"actorID\"]\n",
    "        add_node_properties(nodes, actor_id, properties)\n",
    "        labels[actor_id] = dummies[row['actor_type']]\n",
    "\n",
    "        object_id = row[\"objectID\"]\n",
    "        add_node_properties(nodes, object_id, properties)\n",
    "        labels[object_id] = dummies[row['object']]\n",
    "\n",
    "        edges.append((actor_id, object_id))\n",
    "\n",
    "    features, feat_labels, edge_index, index_map = [], [], [[], []], {}\n",
    "    for node_id, props in nodes.items():\n",
    "        features.append(props)\n",
    "        feat_labels.append(labels[node_id])\n",
    "        index_map[node_id] = len(features) - 1\n",
    "\n",
    "    update_edge_index(edges, edge_index, index_map)\n",
    "\n",
    "    return features, feat_labels, edge_index, list(index_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fmXWs1dKIzD8"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import SAGEConv, GATConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self,in_channel,out_channel):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channel, 32, normalize=True)\n",
    "        self.conv2 = SAGEConv(32, out_channel, normalize=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YBuP_tSq94f4"
   },
   "outputs": [],
   "source": [
    "def visualize(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(h.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3PCP6SXwZaif"
   },
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from multiprocessing import Pool\n",
    "from itertools import compress\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "class EpochSaver(CallbackAny2Vec):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        model.save('word2vec_trace_E3.model')\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "P8oBL8LFaeOf"
   },
   "outputs": [],
   "source": [
    "class EpochLogger(CallbackAny2Vec):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(\"Epoch #{} start\".format(self.epoch))\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(\"Epoch #{} end\".format(self.epoch))\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Se7Ei4tAapVj"
   },
   "outputs": [],
   "source": [
    "logger = EpochLogger()\n",
    "saver = EpochSaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_attributes(d,p):\n",
    "    \n",
    "    f = open(p)\n",
    "    data = [json.loads(x) for x in f if \"EVENT\" in x]\n",
    "\n",
    "    info = []\n",
    "    for x in data:\n",
    "        try:\n",
    "            action = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['type']\n",
    "        except:\n",
    "            action = ''\n",
    "        try:\n",
    "            actor = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['subject']['com.bbn.tc.schema.avro.cdm18.UUID']\n",
    "        except:\n",
    "            actor = ''\n",
    "        try:\n",
    "            obj = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['predicateObject']['com.bbn.tc.schema.avro.cdm18.UUID']\n",
    "        except:\n",
    "            obj = ''\n",
    "        try:\n",
    "            timestamp = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['timestampNanos']\n",
    "        except:\n",
    "            timestamp = ''\n",
    "        try:\n",
    "            cmd = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['properties']['map']['exec']\n",
    "        except:\n",
    "            cmd = ''\n",
    "        try:\n",
    "            path = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['predicateObjectPath']['string']\n",
    "        except:\n",
    "            path = ''\n",
    "        try:\n",
    "            path2 = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['predicateObject2Path']['string']\n",
    "        except:\n",
    "            path2 = ''\n",
    "        try:\n",
    "            obj2 = x['datum']['com.bbn.tc.schema.avro.cdm18.Event']['predicateObject2']['com.bbn.tc.schema.avro.cdm18.UUID']\n",
    "            info.append({'actorID':actor,'objectID':obj2,'action':action,'timestamp':timestamp,'exec':cmd, 'path':path2})\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        info.append({'actorID':actor,'objectID':obj,'action':action,'timestamp':timestamp,'exec':cmd, 'path':path})\n",
    "\n",
    "    rdf = pd.DataFrame.from_records(info).astype(str)\n",
    "    d = d.astype(str)\n",
    "\n",
    "    return d.merge(rdf,how='inner',on=['actorID','objectID','action','timestamp']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Train:\n",
    "    f = open(\"trace_train.txt\")\n",
    "    data = f.read().split('\\n')\n",
    "    data = [line.split('\\t') for line in data]\n",
    "    df = pd.DataFrame (data, columns = ['actorID', 'actor_type','objectID','object','action','timestamp'])\n",
    "    df = df.dropna()\n",
    "    df.sort_values(by='timestamp', ascending=True,inplace=True)\n",
    "    df = add_attributes(df,\"ta1-trace-e3-official-1.json\")\n",
    "    phrases,labels,edges,mapp = prepare_graph(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "p3TAi69zI1bO"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "model = GCN(30,11).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3RDmGME5iPb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0 start\n",
      "Epoch #0 end\n",
      "Epoch #1 start\n",
      "Epoch #1 end\n",
      "Epoch #2 start\n",
      "Epoch #2 end\n",
      "Epoch #3 start\n",
      "Epoch #3 end\n",
      "Epoch #4 start\n",
      "Epoch #4 end\n",
      "Epoch #5 start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5 end\n",
      "Epoch #6 start\n",
      "Epoch #6 end\n",
      "Epoch #7 start\n",
      "Epoch #7 end\n",
      "Epoch #8 start\n",
      "Epoch #8 end\n",
      "Epoch #9 start\n",
      "Epoch #9 end\n",
      "Epoch #10 start\n",
      "Epoch #10 end\n",
      "Epoch #11 start\n",
      "Epoch #11 end\n",
      "Epoch #12 start\n",
      "Epoch #12 end\n",
      "Epoch #13 start\n",
      "Epoch #13 end\n",
      "Epoch #14 start\n",
      "Epoch #14 end\n",
      "Epoch #15 start\n",
      "Epoch #15 end\n",
      "Epoch #16 start\n",
      "Epoch #16 end\n",
      "Epoch #17 start\n",
      "Epoch #17 end\n",
      "Epoch #18 start\n",
      "Epoch #18 end\n",
      "Epoch #19 start\n",
      "Epoch #19 end\n",
      "Epoch #20 start\n",
      "Epoch #20 end\n",
      "Epoch #21 start\n",
      "Epoch #21 end\n",
      "Epoch #22 start\n",
      "Epoch #22 end\n",
      "Epoch #23 start\n",
      "Epoch #23 end\n",
      "Epoch #24 start\n",
      "Epoch #24 end\n",
      "Epoch #25 start\n",
      "Epoch #25 end\n",
      "Epoch #26 start\n",
      "Epoch #26 end\n",
      "Epoch #27 start\n",
      "Epoch #27 end\n",
      "Epoch #28 start\n",
      "Epoch #28 end\n",
      "Epoch #29 start\n",
      "Epoch #29 end\n",
      "Epoch #30 start\n",
      "Epoch #30 end\n",
      "Epoch #31 start\n",
      "Epoch #31 end\n",
      "Epoch #32 start\n",
      "Epoch #32 end\n",
      "Epoch #33 start\n",
      "Epoch #33 end\n",
      "Epoch #34 start\n",
      "Epoch #34 end\n",
      "Epoch #35 start\n",
      "Epoch #35 end\n",
      "Epoch #36 start\n",
      "Epoch #36 end\n",
      "Epoch #37 start\n",
      "Epoch #37 end\n",
      "Epoch #38 start\n",
      "Epoch #38 end\n",
      "Epoch #39 start\n",
      "Epoch #39 end\n",
      "Epoch #40 start\n",
      "Epoch #40 end\n",
      "Epoch #41 start\n",
      "Epoch #41 end\n",
      "Epoch #42 start\n",
      "Epoch #42 end\n",
      "Epoch #43 start\n",
      "Epoch #43 end\n",
      "Epoch #44 start\n",
      "Epoch #44 end\n",
      "Epoch #45 start\n",
      "Epoch #45 end\n",
      "Epoch #46 start\n",
      "Epoch #46 end\n",
      "Epoch #47 start\n",
      "Epoch #47 end\n",
      "Epoch #48 start\n",
      "Epoch #48 end\n",
      "Epoch #49 start\n",
      "Epoch #49 end\n",
      "Epoch #50 start\n",
      "Epoch #50 end\n",
      "Epoch #51 start\n",
      "Epoch #51 end\n",
      "Epoch #52 start\n",
      "Epoch #52 end\n",
      "Epoch #53 start\n",
      "Epoch #53 end\n",
      "Epoch #54 start\n",
      "Epoch #54 end\n",
      "Epoch #55 start\n",
      "Epoch #55 end\n",
      "Epoch #56 start\n",
      "Epoch #56 end\n",
      "Epoch #57 start\n",
      "Epoch #57 end\n",
      "Epoch #58 start\n",
      "Epoch #58 end\n",
      "Epoch #59 start\n",
      "Epoch #59 end\n",
      "Epoch #60 start\n",
      "Epoch #60 end\n",
      "Epoch #61 start\n",
      "Epoch #61 end\n",
      "Epoch #62 start\n",
      "Epoch #62 end\n",
      "Epoch #63 start\n",
      "Epoch #63 end\n",
      "Epoch #64 start\n",
      "Epoch #64 end\n",
      "Epoch #65 start\n",
      "Epoch #65 end\n",
      "Epoch #66 start\n",
      "Epoch #66 end\n",
      "Epoch #67 start\n",
      "Epoch #67 end\n",
      "Epoch #68 start\n",
      "Epoch #68 end\n",
      "Epoch #69 start\n",
      "Epoch #69 end\n",
      "Epoch #70 start\n",
      "Epoch #70 end\n",
      "Epoch #71 start\n",
      "Epoch #71 end\n",
      "Epoch #72 start\n",
      "Epoch #72 end\n",
      "Epoch #73 start\n",
      "Epoch #73 end\n",
      "Epoch #74 start\n",
      "Epoch #74 end\n",
      "Epoch #75 start\n",
      "Epoch #75 end\n",
      "Epoch #76 start\n",
      "Epoch #76 end\n",
      "Epoch #77 start\n",
      "Epoch #77 end\n",
      "Epoch #78 start\n",
      "Epoch #78 end\n",
      "Epoch #79 start\n",
      "Epoch #79 end\n",
      "Epoch #80 start\n",
      "Epoch #80 end\n",
      "Epoch #81 start\n",
      "Epoch #81 end\n",
      "Epoch #82 start\n",
      "Epoch #82 end\n",
      "Epoch #83 start\n",
      "Epoch #83 end\n",
      "Epoch #84 start\n",
      "Epoch #84 end\n",
      "Epoch #85 start\n",
      "Epoch #85 end\n",
      "Epoch #86 start\n",
      "Epoch #86 end\n",
      "Epoch #87 start\n",
      "Epoch #87 end\n",
      "Epoch #88 start\n",
      "Epoch #88 end\n",
      "Epoch #89 start\n",
      "Epoch #89 end\n",
      "Epoch #90 start\n",
      "Epoch #90 end\n",
      "Epoch #91 start\n",
      "Epoch #91 end\n",
      "Epoch #92 start\n",
      "Epoch #92 end\n",
      "Epoch #93 start\n",
      "Epoch #93 end\n",
      "Epoch #94 start\n",
      "Epoch #94 end\n",
      "Epoch #95 start\n",
      "Epoch #95 end\n",
      "Epoch #96 start\n",
      "Epoch #96 end\n",
      "Epoch #97 start\n",
      "Epoch #97 end\n",
      "Epoch #98 start\n",
      "Epoch #98 end\n",
      "Epoch #99 start\n",
      "Epoch #99 end\n",
      "Epoch #100 start\n",
      "Epoch #100 end\n",
      "Epoch #101 start\n",
      "Epoch #101 end\n",
      "Epoch #102 start\n",
      "Epoch #102 end\n",
      "Epoch #103 start\n",
      "Epoch #103 end\n",
      "Epoch #104 start\n",
      "Epoch #104 end\n",
      "Epoch #105 start\n",
      "Epoch #105 end\n",
      "Epoch #106 start\n",
      "Epoch #106 end\n",
      "Epoch #107 start\n",
      "Epoch #107 end\n",
      "Epoch #108 start\n",
      "Epoch #108 end\n",
      "Epoch #109 start\n",
      "Epoch #109 end\n",
      "Epoch #110 start\n",
      "Epoch #110 end\n",
      "Epoch #111 start\n",
      "Epoch #111 end\n",
      "Epoch #112 start\n",
      "Epoch #112 end\n",
      "Epoch #113 start\n",
      "Epoch #113 end\n",
      "Epoch #114 start\n",
      "Epoch #114 end\n",
      "Epoch #115 start\n",
      "Epoch #115 end\n",
      "Epoch #116 start\n",
      "Epoch #116 end\n",
      "Epoch #117 start\n",
      "Epoch #117 end\n",
      "Epoch #118 start\n",
      "Epoch #118 end\n",
      "Epoch #119 start\n",
      "Epoch #119 end\n",
      "Epoch #120 start\n",
      "Epoch #120 end\n",
      "Epoch #121 start\n",
      "Epoch #121 end\n",
      "Epoch #122 start\n",
      "Epoch #122 end\n",
      "Epoch #123 start\n",
      "Epoch #123 end\n",
      "Epoch #124 start\n",
      "Epoch #124 end\n",
      "Epoch #125 start\n",
      "Epoch #125 end\n",
      "Epoch #126 start\n",
      "Epoch #126 end\n",
      "Epoch #127 start\n",
      "Epoch #127 end\n",
      "Epoch #128 start\n",
      "Epoch #128 end\n",
      "Epoch #129 start\n",
      "Epoch #129 end\n",
      "Epoch #130 start\n",
      "Epoch #130 end\n",
      "Epoch #131 start\n",
      "Epoch #131 end\n",
      "Epoch #132 start\n",
      "Epoch #132 end\n",
      "Epoch #133 start\n",
      "Epoch #133 end\n",
      "Epoch #134 start\n",
      "Epoch #134 end\n",
      "Epoch #135 start\n",
      "Epoch #135 end\n",
      "Epoch #136 start\n",
      "Epoch #136 end\n",
      "Epoch #137 start\n",
      "Epoch #137 end\n",
      "Epoch #138 start\n",
      "Epoch #138 end\n",
      "Epoch #139 start\n",
      "Epoch #139 end\n",
      "Epoch #140 start\n",
      "Epoch #140 end\n",
      "Epoch #141 start\n",
      "Epoch #141 end\n",
      "Epoch #142 start\n",
      "Epoch #142 end\n",
      "Epoch #143 start\n",
      "Epoch #143 end\n",
      "Epoch #144 start\n",
      "Epoch #144 end\n",
      "Epoch #145 start\n",
      "Epoch #145 end\n",
      "Epoch #146 start\n",
      "Epoch #146 end\n",
      "Epoch #147 start\n",
      "Epoch #147 end\n",
      "Epoch #148 start\n",
      "Epoch #148 end\n",
      "Epoch #149 start\n",
      "Epoch #149 end\n",
      "Epoch #150 start\n",
      "Epoch #150 end\n",
      "Epoch #151 start\n",
      "Epoch #151 end\n",
      "Epoch #152 start\n",
      "Epoch #152 end\n",
      "Epoch #153 start\n",
      "Epoch #153 end\n",
      "Epoch #154 start\n",
      "Epoch #154 end\n",
      "Epoch #155 start\n",
      "Epoch #155 end\n",
      "Epoch #156 start\n",
      "Epoch #156 end\n",
      "Epoch #157 start\n",
      "Epoch #157 end\n",
      "Epoch #158 start\n",
      "Epoch #158 end\n",
      "Epoch #159 start\n",
      "Epoch #159 end\n",
      "Epoch #160 start\n",
      "Epoch #160 end\n",
      "Epoch #161 start\n",
      "Epoch #161 end\n",
      "Epoch #162 start\n",
      "Epoch #162 end\n",
      "Epoch #163 start\n",
      "Epoch #163 end\n",
      "Epoch #164 start\n",
      "Epoch #164 end\n",
      "Epoch #165 start\n",
      "Epoch #165 end\n",
      "Epoch #166 start\n",
      "Epoch #166 end\n",
      "Epoch #167 start\n",
      "Epoch #167 end\n",
      "Epoch #168 start\n",
      "Epoch #168 end\n",
      "Epoch #169 start\n",
      "Epoch #169 end\n",
      "Epoch #170 start\n",
      "Epoch #170 end\n",
      "Epoch #171 start\n",
      "Epoch #171 end\n",
      "Epoch #172 start\n",
      "Epoch #172 end\n",
      "Epoch #173 start\n",
      "Epoch #173 end\n",
      "Epoch #174 start\n",
      "Epoch #174 end\n",
      "Epoch #175 start\n",
      "Epoch #175 end\n",
      "Epoch #176 start\n",
      "Epoch #176 end\n",
      "Epoch #177 start\n",
      "Epoch #177 end\n",
      "Epoch #178 start\n",
      "Epoch #178 end\n",
      "Epoch #179 start\n",
      "Epoch #179 end\n",
      "Epoch #180 start\n",
      "Epoch #180 end\n",
      "Epoch #181 start\n",
      "Epoch #181 end\n",
      "Epoch #182 start\n",
      "Epoch #182 end\n",
      "Epoch #183 start\n",
      "Epoch #183 end\n",
      "Epoch #184 start\n",
      "Epoch #184 end\n",
      "Epoch #185 start\n",
      "Epoch #185 end\n",
      "Epoch #186 start\n",
      "Epoch #186 end\n",
      "Epoch #187 start\n",
      "Epoch #187 end\n",
      "Epoch #188 start\n",
      "Epoch #188 end\n",
      "Epoch #189 start\n",
      "Epoch #189 end\n",
      "Epoch #190 start\n",
      "Epoch #190 end\n",
      "Epoch #191 start\n",
      "Epoch #191 end\n",
      "Epoch #192 start\n",
      "Epoch #192 end\n",
      "Epoch #193 start\n",
      "Epoch #193 end\n",
      "Epoch #194 start\n",
      "Epoch #194 end\n",
      "Epoch #195 start\n",
      "Epoch #195 end\n",
      "Epoch #196 start\n",
      "Epoch #196 end\n",
      "Epoch #197 start\n",
      "Epoch #197 end\n",
      "Epoch #198 start\n",
      "Epoch #198 end\n",
      "Epoch #199 start\n",
      "Epoch #199 end\n",
      "Epoch #200 start\n",
      "Epoch #200 end\n",
      "Epoch #201 start\n",
      "Epoch #201 end\n",
      "Epoch #202 start\n",
      "Epoch #202 end\n",
      "Epoch #203 start\n",
      "Epoch #203 end\n",
      "Epoch #204 start\n",
      "Epoch #204 end\n",
      "Epoch #205 start\n",
      "Epoch #205 end\n",
      "Epoch #206 start\n",
      "Epoch #206 end\n",
      "Epoch #207 start\n",
      "Epoch #207 end\n",
      "Epoch #208 start\n",
      "Epoch #208 end\n",
      "Epoch #209 start\n",
      "Epoch #209 end\n",
      "Epoch #210 start\n",
      "Epoch #210 end\n",
      "Epoch #211 start\n",
      "Epoch #211 end\n",
      "Epoch #212 start\n",
      "Epoch #212 end\n",
      "Epoch #213 start\n",
      "Epoch #213 end\n",
      "Epoch #214 start\n",
      "Epoch #214 end\n",
      "Epoch #215 start\n",
      "Epoch #215 end\n",
      "Epoch #216 start\n",
      "Epoch #216 end\n",
      "Epoch #217 start\n",
      "Epoch #217 end\n",
      "Epoch #218 start\n",
      "Epoch #218 end\n",
      "Epoch #219 start\n",
      "Epoch #219 end\n",
      "Epoch #220 start\n",
      "Epoch #220 end\n",
      "Epoch #221 start\n",
      "Epoch #221 end\n",
      "Epoch #222 start\n",
      "Epoch #222 end\n",
      "Epoch #223 start\n",
      "Epoch #223 end\n",
      "Epoch #224 start\n",
      "Epoch #224 end\n",
      "Epoch #225 start\n",
      "Epoch #225 end\n",
      "Epoch #226 start\n",
      "Epoch #226 end\n",
      "Epoch #227 start\n",
      "Epoch #227 end\n",
      "Epoch #228 start\n",
      "Epoch #228 end\n",
      "Epoch #229 start\n",
      "Epoch #229 end\n",
      "Epoch #230 start\n",
      "Epoch #230 end\n",
      "Epoch #231 start\n",
      "Epoch #231 end\n",
      "Epoch #232 start\n",
      "Epoch #232 end\n",
      "Epoch #233 start\n",
      "Epoch #233 end\n",
      "Epoch #234 start\n",
      "Epoch #234 end\n",
      "Epoch #235 start\n",
      "Epoch #235 end\n",
      "Epoch #236 start\n",
      "Epoch #236 end\n",
      "Epoch #237 start\n",
      "Epoch #237 end\n",
      "Epoch #238 start\n",
      "Epoch #238 end\n",
      "Epoch #239 start\n",
      "Epoch #239 end\n",
      "Epoch #240 start\n",
      "Epoch #240 end\n",
      "Epoch #241 start\n",
      "Epoch #241 end\n",
      "Epoch #242 start\n",
      "Epoch #242 end\n",
      "Epoch #243 start\n",
      "Epoch #243 end\n",
      "Epoch #244 start\n",
      "Epoch #244 end\n",
      "Epoch #245 start\n",
      "Epoch #245 end\n",
      "Epoch #246 start\n",
      "Epoch #246 end\n",
      "Epoch #247 start\n",
      "Epoch #247 end\n",
      "Epoch #248 start\n",
      "Epoch #248 end\n",
      "Epoch #249 start\n",
      "Epoch #249 end\n",
      "Epoch #250 start\n",
      "Epoch #250 end\n",
      "Epoch #251 start\n",
      "Epoch #251 end\n",
      "Epoch #252 start\n",
      "Epoch #252 end\n",
      "Epoch #253 start\n",
      "Epoch #253 end\n",
      "Epoch #254 start\n",
      "Epoch #254 end\n",
      "Epoch #255 start\n",
      "Epoch #255 end\n",
      "Epoch #256 start\n",
      "Epoch #256 end\n",
      "Epoch #257 start\n",
      "Epoch #257 end\n",
      "Epoch #258 start\n",
      "Epoch #258 end\n",
      "Epoch #259 start\n",
      "Epoch #259 end\n",
      "Epoch #260 start\n",
      "Epoch #260 end\n",
      "Epoch #261 start\n",
      "Epoch #261 end\n",
      "Epoch #262 start\n",
      "Epoch #262 end\n",
      "Epoch #263 start\n",
      "Epoch #263 end\n",
      "Epoch #264 start\n",
      "Epoch #264 end\n",
      "Epoch #265 start\n",
      "Epoch #265 end\n",
      "Epoch #266 start\n",
      "Epoch #266 end\n",
      "Epoch #267 start\n",
      "Epoch #267 end\n",
      "Epoch #268 start\n",
      "Epoch #268 end\n",
      "Epoch #269 start\n",
      "Epoch #269 end\n",
      "Epoch #270 start\n",
      "Epoch #270 end\n",
      "Epoch #271 start\n",
      "Epoch #271 end\n",
      "Epoch #272 start\n",
      "Epoch #272 end\n",
      "Epoch #273 start\n",
      "Epoch #273 end\n",
      "Epoch #274 start\n",
      "Epoch #274 end\n",
      "Epoch #275 start\n",
      "Epoch #275 end\n",
      "Epoch #276 start\n",
      "Epoch #276 end\n",
      "Epoch #277 start\n",
      "Epoch #277 end\n",
      "Epoch #278 start\n",
      "Epoch #278 end\n",
      "Epoch #279 start\n",
      "Epoch #279 end\n",
      "Epoch #280 start\n",
      "Epoch #280 end\n",
      "Epoch #281 start\n",
      "Epoch #281 end\n",
      "Epoch #282 start\n",
      "Epoch #282 end\n",
      "Epoch #283 start\n",
      "Epoch #283 end\n",
      "Epoch #284 start\n",
      "Epoch #284 end\n",
      "Epoch #285 start\n",
      "Epoch #285 end\n",
      "Epoch #286 start\n",
      "Epoch #286 end\n",
      "Epoch #287 start\n",
      "Epoch #287 end\n",
      "Epoch #288 start\n",
      "Epoch #288 end\n",
      "Epoch #289 start\n",
      "Epoch #289 end\n",
      "Epoch #290 start\n",
      "Epoch #290 end\n",
      "Epoch #291 start\n",
      "Epoch #291 end\n",
      "Epoch #292 start\n",
      "Epoch #292 end\n",
      "Epoch #293 start\n",
      "Epoch #293 end\n",
      "Epoch #294 start\n",
      "Epoch #294 end\n",
      "Epoch #295 start\n",
      "Epoch #295 end\n",
      "Epoch #296 start\n",
      "Epoch #296 end\n",
      "Epoch #297 start\n",
      "Epoch #297 end\n",
      "Epoch #298 start\n",
      "Epoch #298 end\n",
      "Epoch #299 start\n",
      "Epoch #299 end\n"
     ]
    }
   ],
   "source": [
    "if Train:\n",
    "    word2vec = Word2Vec(sentences=phrases, vector_size=30, window=5, min_count=1, workers=8,epochs=300,callbacks=[saver,logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Vn_pMyt5Jd-6"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "class PositionalEncoder:\n",
    "\n",
    "    def __init__(self, d_model, max_len=100000):\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        self.pe = torch.zeros(max_len, d_model)\n",
    "        self.pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    def embed(self, x):\n",
    "        return x + self.pe[:x.size(0)]\n",
    "\n",
    "\n",
    "def infer(document):\n",
    "    word_embeddings = [w2vmodel.wv[word] for word in document if word in  w2vmodel.wv]\n",
    "    \n",
    "    if not word_embeddings:\n",
    "        return np.zeros(20)\n",
    "\n",
    "    output_embedding = torch.tensor(word_embeddings, dtype=torch.float)\n",
    "    if len(document) < 100000:\n",
    "        output_embedding = encoder.embed(output_embedding)\n",
    "\n",
    "    output_embedding = output_embedding.detach().cpu().numpy()\n",
    "    return np.mean(output_embedding, axis=0)\n",
    "\n",
    "encoder = PositionalEncoder(30)\n",
    "w2vmodel = Word2Vec.load(\"trained_weights/trace/word2vec_trace_E3.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 503992,
     "status": "ok",
     "timestamp": 1673553696499,
     "user": {
      "displayName": "Mati Ur Rehman",
      "userId": "04281203290774044297"
     },
     "user_tz": 300
    },
    "id": "Gclj6HVL17lD",
    "outputId": "ae20b63a-34d6-45ac-a91e-d659f8fc957f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.289345050356903\n",
      "Model# 0. 350102 nodes still misclassified \n",
      "\n",
      "2.295717938222219\n",
      "Model# 1. 333572 nodes still misclassified \n",
      "\n",
      "2.288134466906616\n",
      "Model# 2. 151987 nodes still misclassified \n",
      "\n",
      "2.298121496990221\n",
      "Model# 3. 88500 nodes still misclassified \n",
      "\n",
      "2.3115988666728393\n",
      "Model# 4. 82529 nodes still misclassified \n",
      "\n",
      "2.306044773394557\n",
      "Model# 5. 55992 nodes still misclassified \n",
      "\n",
      "2.30281624284399\n",
      "Model# 6. 45608 nodes still misclassified \n",
      "\n",
      "2.3020153665015664\n",
      "Model# 7. 42186 nodes still misclassified \n",
      "\n",
      "2.2972994435649055\n",
      "Model# 8. 31344 nodes still misclassified \n",
      "\n",
      "2.294313113503215\n",
      "Model# 9. 29793 nodes still misclassified \n",
      "\n",
      "2.290704889679685\n",
      "Model# 10. 29682 nodes still misclassified \n",
      "\n",
      "2.2898686137307678\n",
      "Model# 11. 29613 nodes still misclassified \n",
      "\n",
      "2.2883243541790312\n",
      "Model# 12. 28902 nodes still misclassified \n",
      "\n",
      "2.2859424554009014\n",
      "Model# 13. 28450 nodes still misclassified \n",
      "\n",
      "2.2846722296964517\n",
      "Model# 14. 27267 nodes still misclassified \n",
      "\n",
      "2.284278520355787\n",
      "Model# 15. 24481 nodes still misclassified \n",
      "\n",
      "2.2798950632025297\n",
      "Model# 16. 22959 nodes still misclassified \n",
      "\n",
      "2.278296146135701\n",
      "Model# 17. 21767 nodes still misclassified \n",
      "\n",
      "2.278679742774555\n",
      "Model# 18. 21017 nodes still misclassified \n",
      "\n",
      "2.275652370427016\n",
      "Model# 19. 20882 nodes still misclassified \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric import utils\n",
    "\n",
    "if Train:\n",
    "    l = np.array(labels)\n",
    "    class_weights = class_weight.compute_class_weight(class_weight = None,classes = np.unique(l),y = l)\n",
    "    class_weights = torch.tensor(class_weights,dtype=torch.float).to(device)\n",
    "    criterion = CrossEntropyLoss(weight=class_weights,reduction='mean')\n",
    "\n",
    "    nodes = [infer(x) for x in phrases]\n",
    "    nodes = np.array(nodes)  \n",
    "\n",
    "    graph = Data(x=torch.tensor(nodes,dtype=torch.float).to(device),y=torch.tensor(labels,dtype=torch.long).to(device), edge_index=torch.tensor(edges,dtype=torch.long).to(device))\n",
    "    graph.n_id = torch.arange(graph.num_nodes)\n",
    "    mask = torch.tensor([True]*graph.num_nodes, dtype=torch.bool)\n",
    "\n",
    "    for m_n in range(20):\n",
    "\n",
    "      loader = NeighborLoader(graph, num_neighbors=[-1,-1], batch_size=5000,input_nodes=mask)\n",
    "      total_loss = 0\n",
    "      for subg in loader:\n",
    "          model.train()\n",
    "          optimizer.zero_grad() \n",
    "          out = model(subg.x, subg.edge_index) \n",
    "          loss = criterion(out, subg.y) \n",
    "          loss.backward() \n",
    "          optimizer.step()      \n",
    "          total_loss += loss.item() * subg.batch_size\n",
    "      print(total_loss / mask.sum().item())\n",
    "\n",
    "      loader = NeighborLoader(graph, num_neighbors=[-1,-1], batch_size=5000,input_nodes=mask)\n",
    "      for subg in loader:\n",
    "          model.eval()\n",
    "          out = model(subg.x, subg.edge_index)\n",
    "\n",
    "          sorted, indices = out.sort(dim=1,descending=True)\n",
    "          conf = (sorted[:,0] - sorted[:,1]) / sorted[:,0]\n",
    "          conf = (conf - conf.min()) / conf.max()\n",
    "\n",
    "          pred = indices[:,0]\n",
    "          cond = (pred == subg.y) & (conf >= 0.9)\n",
    "          mask[subg.n_id[cond]] = False\n",
    "\n",
    "      torch.save(model.state_dict(), f'lword2vec_gnn_trace{m_n}_E3.pth')\n",
    "      print(f'Model# {m_n}. {mask.sum().item()} nodes still misclassified \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "from torch_geometric import utils\n",
    "\n",
    "def Get_Adjacent(ids, mapp, edges, hops):\n",
    "    if hops == 0:\n",
    "        return set()\n",
    "    \n",
    "    neighbors = set()\n",
    "    for edge in zip(edges[0], edges[1]):\n",
    "        if any(mapp[node] in ids for node in edge):\n",
    "            neighbors.update(mapp[node] for node in edge)\n",
    "\n",
    "    if hops > 1:\n",
    "        neighbors = neighbors.union(Get_Adjacent(neighbors, mapp, edges, hops - 1))\n",
    "    \n",
    "    return neighbors\n",
    "\n",
    "def calculate_metrics(TP, FP, FN, TN):\n",
    "    FPR = FP / (FP + TN) if FP + TN > 0 else 0\n",
    "    TPR = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "\n",
    "    prec = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    rec = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    fscore = (2 * prec * rec) / (prec + rec) if prec + rec > 0 else 0\n",
    "\n",
    "    return prec, rec, fscore, FPR, TPR\n",
    "\n",
    "def helper(MP, all_pids, GP, edges, mapp):\n",
    "    TP = MP.intersection(GP)\n",
    "    FP = MP - GP\n",
    "    FN = GP - MP\n",
    "    TN = all_pids - (GP | MP)\n",
    "\n",
    "    two_hop_gp = Get_Adjacent(GP, mapp, edges, 2)\n",
    "    two_hop_tp = Get_Adjacent(TP, mapp, edges, 2)\n",
    "    FPL = FP - two_hop_gp\n",
    "    TPL = TP.union(FN.intersection(two_hop_tp))\n",
    "    FN = FN - two_hop_tp\n",
    "\n",
    "    TP, FP, FN, TN = len(TPL), len(FPL), len(FN), len(TN)\n",
    "\n",
    "    prec, rec, fscore, FPR, TPR = calculate_metrics(TP, FP, FN, TN)\n",
    "    print(f\"True Positives: {TP}, False Positives: {FP}, False Negatives: {FN}\")\n",
    "    print(f\"Precision: {round(prec, 4)}, Recall: {round(rec, 4)}, Fscore: {round(fscore, 4)}\")\n",
    "    print(f\"True Negatives: {TN}\")\n",
    "    \n",
    "    return TPL, FPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "OZFrSLVZ29qU"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "f = open(\"trace_test.txt\")\n",
    "data = f.read().split('\\n')\n",
    "data = [line.split('\\t') for line in data]\n",
    "df = pd.DataFrame (data, columns = ['actorID', 'actor_type','objectID','object','action','timestamp'])\n",
    "df = df.dropna()\n",
    "df.sort_values(by='timestamp', ascending=True,inplace=True)\n",
    "\n",
    "df = add_attributes(df,\"ta1-trace-e3-official-1.json.4\")\n",
    "  \n",
    "with open(\"data_files/trace.json\", \"r\") as json_file:\n",
    "    GT_mal = set(json.load(json_file))\n",
    "\n",
    "data = df\n",
    "\n",
    "phrases,labels,edges,mapp = prepare_graph(data)\n",
    "nodes = [infer(x) for x in phrases]\n",
    "nodes = np.array(nodes)  \n",
    "\n",
    "all_ids = list(data['actorID']) + list(data['objectID'])\n",
    "all_ids = set(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 67383, False Positives: 3477, False Negatives: 790\n",
      "Precision: 0.9509, Recall: 0.9884, Fscore: 0.9693\n",
      "True Negatives: 1109469\n"
     ]
    }
   ],
   "source": [
    "graph = Data(x=torch.tensor(nodes,dtype=torch.float).to(device),y=torch.tensor(labels,dtype=torch.long).to(device), edge_index=torch.tensor(edges,dtype=torch.long).to(device))\n",
    "graph.n_id = torch.arange(graph.num_nodes)\n",
    "flag = torch.tensor([True]*graph.num_nodes, dtype=torch.bool)\n",
    "\n",
    "for m_n in range(10):\n",
    "  model.load_state_dict(torch.load(f'trained_weights/trace/lword2vec_gnn_trace{m_n}_E3.pth'))\n",
    "  loader = NeighborLoader(graph, num_neighbors=[-1,-1], batch_size=5000)    \n",
    "  for subg in loader:\n",
    "      model.eval()\n",
    "      out = model(subg.x, subg.edge_index)\n",
    "\n",
    "      sorted, indices = out.sort(dim=1,descending=True)\n",
    "      conf = (sorted[:,0] - sorted[:,1]) / sorted[:,0]\n",
    "      conf = (conf - conf.min()) / conf.max()\n",
    "    \n",
    "      pred = indices[:,0]\n",
    "      cond = (pred == subg.y)# & (conf >= 0.3)\n",
    "      flag[subg.n_id[cond]] = torch.logical_and(flag[subg.n_id[cond]], torch.tensor([False]*len(flag[subg.n_id[cond]]), dtype=torch.bool))\n",
    "\n",
    "index = utils.mask_to_index(flag).tolist()\n",
    "ids = set([mapp[x] for x in index])\n",
    "alerts = helper(set(ids),set(all_ids),GT_mal,edges,mapp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse(ids, mapping, edges, hops, visited=None):\n",
    "    if hops == 0:\n",
    "        return set()\n",
    "\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "\n",
    "    neighbors = set()\n",
    "    for src, dst in zip(edges[0], edges[1]):\n",
    "        src_mapped, dst_mapped = mapping[src], mapping[dst]\n",
    "\n",
    "        if (src_mapped in ids and dst_mapped not in visited) or \\\n",
    "           (dst_mapped in ids and src_mapped not in visited):\n",
    "            neighbors.add(src_mapped)\n",
    "            neighbors.add(dst_mapped)\n",
    "\n",
    "        visited.add(src_mapped)\n",
    "        visited.add(dst_mapped)\n",
    "\n",
    "    neighbors.difference_update(ids) \n",
    "    return ids.union(traverse(neighbors, mapping, edges, hops - 1, visited))\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def find_connected_alerts(start_alert, mapping, edges, depth, remaining_alerts):\n",
    "    connected_path = traverse({start_alert}, mapping, edges, depth)\n",
    "    return connected_path.intersection(remaining_alerts)\n",
    "\n",
    "def generate_incident_graphs(alerts, edges, mapping, depth):\n",
    "    incident_graphs = []\n",
    "    remaining_alerts = set(alerts)\n",
    "\n",
    "    while remaining_alerts:\n",
    "        alert = remaining_alerts.pop()\n",
    "        connected_alerts = find_connected_alerts(alert, mapping, edges, depth, remaining_alerts)\n",
    "\n",
    "        if len(connected_alerts) > 1:\n",
    "            incident_graphs.append(connected_alerts)\n",
    "            remaining_alerts -= connected_alerts\n",
    "\n",
    "    return incident_graphs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "flash_amaan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
